[
  {
    "paper": {
      "id": "2602.20159",
      "authors": [
        {
          "_id": "699d1e7a4e37ec6dfa1bc5b7",
          "name": "Maijunxian Wang",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5b8",
          "name": "Ruisi Wang",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5b9",
          "name": "Juyi Lin",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5ba",
          "name": "Ran Ji",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5bb",
          "name": "Thaddäus Wiedemer",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5bc",
          "name": "Qingying Gao",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5bd",
          "name": "Dezhi Luo",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5be",
          "name": "Yaoyao Qian",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5bf",
          "name": "Lianyu Huang",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5c0",
          "name": "Zelong Hong",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5c1",
          "name": "Jiahui Ge",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5c2",
          "name": "Qianli Ma",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5c3",
          "name": "Hang He",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5c4",
          "name": "Yifan Zhou",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5c5",
          "name": "Lingzi Guo",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5c6",
          "name": "Lantao Mei",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5c7",
          "name": "Jiachen Li",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5c8",
          "name": "Hanwen Xing",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5c9",
          "name": "Tianqi Zhao",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5ca",
          "name": "Fengyuan Yu",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5cb",
          "name": "Weihang Xiao",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5cc",
          "name": "Yizheng Jiao",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5cd",
          "name": "Jianheng Hou",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5ce",
          "name": "Danyang Zhang",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5cf",
          "name": "Pengcheng Xu",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5d0",
          "name": "Boyang Zhong",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5d1",
          "name": "Zehong Zhao",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5d2",
          "name": "Gaoyun Fang",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5d3",
          "name": "John Kitaoka",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5d4",
          "name": "Yile Xu",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5d5",
          "name": "Hua Xu",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5d6",
          "name": "Kenton Blacutt",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5d7",
          "name": "Tin Nguyen",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5d8",
          "name": "Siyuan Song",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5d9",
          "name": "Haoran Sun",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5da",
          "name": "Shaoyue Wen",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5db",
          "name": "Linyang He",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5dc",
          "name": "Runming Wang",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5dd",
          "name": "Yanzhi Wang",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5de",
          "name": "Mengyue Yang",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5df",
          "name": "Ziqiao Ma",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5e0",
          "name": "Raphaël Millière",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5e1",
          "name": "Freda Shi",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5e2",
          "name": "Nuno Vasconcelos",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5e3",
          "name": "Daniel Khashabi",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5e4",
          "name": "Alan Yuille",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5e5",
          "name": "Yilun Du",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5e6",
          "name": "Ziming Liu",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5e7",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5e8",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5e9",
          "name": "Ziwei Liu",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5ea",
          "name": "Vikash Kumar",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5eb",
          "name": "Yijiang Li",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5ec",
          "name": "Lei Yang",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5ed",
          "name": "Zhongang Cai",
          "hidden": false
        },
        {
          "_id": "699d1e7a4e37ec6dfa1bc5ee",
          "name": "Hokin Deng",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IjK1EcVGF8x-SG3ymPI6T.mp4"
      ],
      "publishedAt": "2026-02-23T18:59:41.000Z",
      "submittedOnDailyAt": "2026-02-24T01:14:31.428Z",
      "title": "A Very Big Video Reasoning Suite",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .",
      "upvotes": 53,
      "discussionId": "699d1e7b4e37ec6dfa1bc5ef",
      "projectPage": "https://video-reason.com/",
      "ai_summary": "A large-scale video reasoning dataset and benchmark are introduced to study video intelligence capabilities beyond visual quality, enabling systematic analysis of spatiotemporal reasoning and generalization across diverse tasks.",
      "ai_keywords": [
        "video reasoning",
        "spatiotemporal consistency",
        "emergent generalization",
        "video reasoning benchmark",
        "video reasoning dataset"
      ]
    },
    "publishedAt": "2026-02-23T13:59:41.000Z",
    "title": "A Very Big Video Reasoning Suite",
    "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IjK1EcVGF8x-SG3ymPI6T.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20159.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 237,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.19313",
      "authors": [
        {
          "_id": "699d1f754e37ec6dfa1bc5ff",
          "name": "Shirui Chen",
          "hidden": false
        },
        {
          "_id": "699d1f754e37ec6dfa1bc600",
          "name": "Cole Harrison",
          "hidden": false
        },
        {
          "_id": "699d1f754e37ec6dfa1bc601",
          "name": "Ying-Chun Lee",
          "hidden": false
        },
        {
          "_id": "699d1f754e37ec6dfa1bc602",
          "name": "Angela Jin Yang",
          "hidden": false
        },
        {
          "_id": "699d1f754e37ec6dfa1bc603",
          "name": "Zhongzheng Ren",
          "hidden": false
        },
        {
          "_id": "699d1f754e37ec6dfa1bc604",
          "name": "Lillian J. Ratliff",
          "hidden": false
        },
        {
          "_id": "699d1f754e37ec6dfa1bc605",
          "name": "Jiafei Duan",
          "hidden": false
        },
        {
          "_id": "699d1f754e37ec6dfa1bc606",
          "name": "Dieter Fox",
          "hidden": false
        },
        {
          "_id": "699d1f754e37ec6dfa1bc607",
          "name": "Ranjay Krishna",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/ez9whqUgnhTTjrPZiPdzP.png"
      ],
      "publishedAt": "2026-02-22T19:25:48.000Z",
      "submittedOnDailyAt": "2026-02-24T01:19:53.486Z",
      "title": "TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics",
      "submittedOnDailyBy": {
        "_id": "632b42626110e37dba3d5bcb",
        "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg",
        "isPro": false,
        "fullname": "Duan",
        "user": "Jiafei1224",
        "type": "user"
      },
      "summary": "While Vision-Language-Action (VLA) models have seen rapid progress in pretraining, their advancement in Reinforcement Learning (RL) remains hampered by low sample efficiency and sparse rewards in real-world settings. Developing generalizable process reward models is essential for providing the fine-grained feedback necessary to bridge this gap, yet existing temporal value functions often fail to generalize beyond their training domains. We introduce TOPReward, a novel, probabilistically grounded temporal value function that leverages the latent world knowledge of pretrained video Vision-Language Models (VLMs) to estimate robotic task progress. Unlike prior methods that prompt VLMs to directly output progress values, which are prone to numerical misrepresentation, TOPReward extracts task progress directly from the VLM's internal token logits. In zero-shot evaluations across 130+ distinct real-world tasks and multiple robot platforms (e.g., Franka, YAM, SO-100/101), TOPReward achieves 0.947 mean Value-Order Correlation (VOC) on Qwen3-VL, dramatically outperforming the state-of-the-art GVL baseline which achieves near-zero correlation on the same open-source model. We further demonstrate that TOPReward serves as a versatile tool for downstream applications, including success detection and reward-aligned behavior cloning.",
      "upvotes": 14,
      "discussionId": "699d1f754e37ec6dfa1bc608",
      "projectPage": "https://topreward.github.io/webpage/",
      "githubRepo": "https://github.com/TOPReward/TOPReward",
      "githubRepoAddedBy": "user",
      "ai_summary": "TOPReward is a probabilistically grounded temporal value function that uses pretrained video Vision-Language Models to estimate robotic task progress through internal token logits, achieving superior performance in zero-shot evaluations across diverse real-world tasks.",
      "ai_keywords": [
        "Vision-Language-Action models",
        "Reinforcement Learning",
        "temporal value functions",
        "Vision-Language Models",
        "token logits",
        "Value-Order Correlation",
        "reward-aligned behavior cloning"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "5e70f3648ce3c604d78fe132",
        "name": "allenai",
        "fullname": "Ai2",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"
      }
    },
    "publishedAt": "2026-02-22T14:25:48.000Z",
    "title": "TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics",
    "summary": "While Vision-Language-Action (VLA) models have seen rapid progress in pretraining, their advancement in Reinforcement Learning (RL) remains hampered by low sample efficiency and sparse rewards in real-world settings. Developing generalizable process reward models is essential for providing the fine-grained feedback necessary to bridge this gap, yet existing temporal value functions often fail to generalize beyond their training domains. We introduce TOPReward, a novel, probabilistically grounded temporal value function that leverages the latent world knowledge of pretrained video Vision-Language Models (VLMs) to estimate robotic task progress. Unlike prior methods that prompt VLMs to directly output progress values, which are prone to numerical misrepresentation, TOPReward extracts task progress directly from the VLM's internal token logits. In zero-shot evaluations across 130+ distinct real-world tasks and multiple robot platforms (e.g., Franka, YAM, SO-100/101), TOPReward achieves 0.947 mean Value-Order Correlation (VOC) on Qwen3-VL, dramatically outperforming the state-of-the-art GVL baseline which achieves near-zero correlation on the same open-source model. We further demonstrate that TOPReward serves as a versatile tool for downstream applications, including success detection and reward-aligned behavior cloning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/ez9whqUgnhTTjrPZiPdzP.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.19313.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632b42626110e37dba3d5bcb",
      "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg",
      "fullname": "Duan",
      "name": "Jiafei1224",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5e70f3648ce3c604d78fe132",
      "name": "allenai",
      "fullname": "Ai2",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.20093",
      "authors": [
        {
          "_id": "699d5e5a4e37ec6dfa1bc6ef",
          "name": "Kun Yang",
          "hidden": false
        },
        {
          "_id": "699d5e5a4e37ec6dfa1bc6f0",
          "name": "Yuxuan Zhu",
          "hidden": false
        },
        {
          "_id": "699d5e5a4e37ec6dfa1bc6f1",
          "name": "Yazhe Chen",
          "hidden": false
        },
        {
          "_id": "699d5e5a4e37ec6dfa1bc6f2",
          "name": "Siyao Zheng",
          "hidden": false
        },
        {
          "_id": "699d5e5a4e37ec6dfa1bc6f3",
          "name": "Bangyang Hong",
          "hidden": false
        },
        {
          "_id": "699d5e5a4e37ec6dfa1bc6f4",
          "name": "Kangle Wu",
          "hidden": false
        },
        {
          "_id": "699d5e5a4e37ec6dfa1bc6f5",
          "name": "Yabo Ni",
          "hidden": false
        },
        {
          "_id": "699d5e5a4e37ec6dfa1bc6f6",
          "name": "Anxiang Zeng",
          "hidden": false
        },
        {
          "_id": "699d5e5a4e37ec6dfa1bc6f7",
          "name": "Cong Fu",
          "hidden": false
        },
        {
          "_id": "699d5e5a4e37ec6dfa1bc6f8",
          "name": "Hui Li",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-23T18:02:50.000Z",
      "submittedOnDailyAt": "2026-02-24T05:47:10.522Z",
      "title": "ManCAR: Manifold-Constrained Latent Reasoning with Adaptive Test-Time Computation for Sequential Recommendation",
      "submittedOnDailyBy": {
        "_id": "665e8515045fcbf12b99558a",
        "avatarUrl": "/avatars/e4ea26703d9dd2c8f19412ccd81d2cae.svg",
        "isPro": false,
        "fullname": "Fu Cong",
        "user": "fcthebrave",
        "type": "user"
      },
      "summary": "Sequential recommendation increasingly employs latent multi-step reasoning to enhance test-time computation. Despite empirical gains, existing approaches largely drive intermediate reasoning states via target-dominant objectives without imposing explicit feasibility constraints. This results in latent drift, where reasoning trajectories deviate into implausible regions. We argue that effective recommendation reasoning should instead be viewed as navigation on a collaborative manifold rather than free-form latent refinement. To this end, we propose ManCAR (Manifold-Constrained Adaptive Reasoning), a principled framework that grounds reasoning within the topology of a global interaction graph. ManCAR constructs a local intent prior from the collaborative neighborhood of a user's recent actions, represented as a distribution over the item simplex. During training, the model progressively aligns its latent predictive distribution with this prior, forcing the reasoning trajectory to remain within the valid manifold. At test time, reasoning proceeds adaptively until the predictive distribution stabilizes, avoiding over-refinement. We provide a variational interpretation of ManCAR to theoretically validate its drift-prevention and adaptive test-time stopping mechanisms. Experiments on seven benchmarks demonstrate that ManCAR consistently outperforms state-of-the-art baselines, achieving up to a 46.88% relative improvement w.r.t. NDCG@10. Our code is available at https://github.com/FuCongResearchSquad/ManCAR.",
      "upvotes": 13,
      "discussionId": "699d5e5a4e37ec6dfa1bc6f9",
      "githubRepo": "https://github.com/FuCongResearchSquad/ManCAR",
      "githubRepoAddedBy": "user",
      "ai_summary": "ManCAR is a recommendation framework that constrains latent reasoning within a collaborative manifold to prevent implausible trajectories and improve accuracy.",
      "ai_keywords": [
        "latent multi-step reasoning",
        "latent drift",
        "collaborative manifold",
        "global interaction graph",
        "local intent prior",
        "item simplex",
        "variational interpretation",
        "adaptive reasoning",
        "test-time computation",
        "NDCG@10"
      ],
      "organization": {
        "_id": "693bcc3aca53da32fcf95a61",
        "name": "PIIR",
        "fullname": "Personalized&Inclusive Intelligence Research"
      }
    },
    "publishedAt": "2026-02-23T13:02:50.000Z",
    "title": "ManCAR: Manifold-Constrained Latent Reasoning with Adaptive Test-Time Computation for Sequential Recommendation",
    "summary": "Sequential recommendation increasingly employs latent multi-step reasoning to enhance test-time computation. Despite empirical gains, existing approaches largely drive intermediate reasoning states via target-dominant objectives without imposing explicit feasibility constraints. This results in latent drift, where reasoning trajectories deviate into implausible regions. We argue that effective recommendation reasoning should instead be viewed as navigation on a collaborative manifold rather than free-form latent refinement. To this end, we propose ManCAR (Manifold-Constrained Adaptive Reasoning), a principled framework that grounds reasoning within the topology of a global interaction graph. ManCAR constructs a local intent prior from the collaborative neighborhood of a user's recent actions, represented as a distribution over the item simplex. During training, the model progressively aligns its latent predictive distribution with this prior, forcing the reasoning trajectory to remain within the valid manifold. At test time, reasoning proceeds adaptively until the predictive distribution stabilizes, avoiding over-refinement. We provide a variational interpretation of ManCAR to theoretically validate its drift-prevention and adaptive test-time stopping mechanisms. Experiments on seven benchmarks demonstrate that ManCAR consistently outperforms state-of-the-art baselines, achieving up to a 46.88% relative improvement w.r.t. NDCG@10. Our code is available at https://github.com/FuCongResearchSquad/ManCAR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20093.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "665e8515045fcbf12b99558a",
      "avatarUrl": "/avatars/e4ea26703d9dd2c8f19412ccd81d2cae.svg",
      "fullname": "Fu Cong",
      "name": "fcthebrave",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "693bcc3aca53da32fcf95a61",
      "name": "PIIR",
      "fullname": "Personalized&Inclusive Intelligence Research"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.20161",
      "authors": [
        {
          "_id": "699d14864e37ec6dfa1bc503",
          "name": "Abdelrahman Shaker",
          "hidden": false
        },
        {
          "_id": "699d14864e37ec6dfa1bc504",
          "name": "Ahmed Heakl",
          "hidden": false
        },
        {
          "_id": "699d14864e37ec6dfa1bc505",
          "name": "Jaseel Muhammad",
          "hidden": false
        },
        {
          "_id": "699d14864e37ec6dfa1bc506",
          "name": "Ritesh Thawkar",
          "hidden": false
        },
        {
          "_id": "699d14864e37ec6dfa1bc507",
          "name": "Omkar Thawakar",
          "hidden": false
        },
        {
          "_id": "699d14864e37ec6dfa1bc508",
          "name": "Senmao Li",
          "hidden": false
        },
        {
          "_id": "699d14864e37ec6dfa1bc509",
          "name": "Hisham Cholakkal",
          "hidden": false
        },
        {
          "_id": "699d14864e37ec6dfa1bc50a",
          "name": "Ian Reid",
          "hidden": false
        },
        {
          "_id": "699d14864e37ec6dfa1bc50b",
          "name": "Eric P. Xing",
          "hidden": false
        },
        {
          "_id": "699d14864e37ec6dfa1bc50c",
          "name": "Salman Khan",
          "hidden": false
        },
        {
          "_id": "699d14864e37ec6dfa1bc50d",
          "name": "Fahad Shahbaz Khan",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-23T18:59:58.000Z",
      "submittedOnDailyAt": "2026-02-24T00:33:58.107Z",
      "title": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device",
      "submittedOnDailyBy": {
        "_id": "656864e12d73834278a8dea7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
        "isPro": true,
        "fullname": "Ahmed Heakl",
        "user": "ahmedheakl",
        "type": "user"
      },
      "summary": "Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/",
      "upvotes": 12,
      "discussionId": "699d14864e37ec6dfa1bc50e",
      "projectPage": "https://amshaker.github.io/Mobile-O/",
      "githubRepo": "https://github.com/Amshaker/Mobile-O",
      "githubRepoAddedBy": "user",
      "ai_summary": "A compact vision-language-diffusion model called Mobile-O enables efficient unified multimodal understanding and generation on mobile devices through specialized architecture design and optimized training methodology.",
      "ai_keywords": [
        "vision-language-diffusion model",
        "Mobile Conditioning Projector",
        "depthwise-separable convolutions",
        "layerwise alignment",
        "cross-modal conditioning",
        "diffusion generator",
        "unified multimodal intelligence",
        "real-time processing",
        "edge devices",
        "visual understanding",
        "visual generation"
      ],
      "githubStars": 3,
      "organization": {
        "_id": "61fb9e24dc607a42af5f193f",
        "name": "MBZUAI",
        "fullname": "Mohamed Bin Zayed University of Artificial Intelligence",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643879908583-603ab5664a944b99e81476e8.jpeg"
      }
    },
    "publishedAt": "2026-02-23T13:59:58.000Z",
    "title": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device",
    "summary": "Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20161.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "656864e12d73834278a8dea7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
      "fullname": "Ahmed Heakl",
      "name": "ahmedheakl",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 61,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "61fb9e24dc607a42af5f193f",
      "name": "MBZUAI",
      "fullname": "Mohamed Bin Zayed University of Artificial Intelligence",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643879908583-603ab5664a944b99e81476e8.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.19895",
      "authors": [
        {
          "_id": "699d1f054e37ec6dfa1bc5f1",
          "name": "Zhongwei Wan",
          "hidden": false
        },
        {
          "_id": "699d1f054e37ec6dfa1bc5f2",
          "name": "Yun Shen",
          "hidden": false
        },
        {
          "_id": "699d1f054e37ec6dfa1bc5f3",
          "name": "Zhihao Dou",
          "hidden": false
        },
        {
          "_id": "699d1f054e37ec6dfa1bc5f4",
          "name": "Donghao Zhou",
          "hidden": false
        },
        {
          "_id": "699d1f054e37ec6dfa1bc5f5",
          "name": "Yu Zhang",
          "hidden": false
        },
        {
          "_id": "699d1f054e37ec6dfa1bc5f6",
          "name": "Xin Wang",
          "hidden": false
        },
        {
          "_id": "699d1f054e37ec6dfa1bc5f7",
          "name": "Hui Shen",
          "hidden": false
        },
        {
          "_id": "699d1f054e37ec6dfa1bc5f8",
          "name": "Jing Xiong",
          "hidden": false
        },
        {
          "_id": "699d1f054e37ec6dfa1bc5f9",
          "name": "Chaofan Tao",
          "hidden": false
        },
        {
          "_id": "699d1f054e37ec6dfa1bc5fa",
          "name": "Zixuan Zhong",
          "hidden": false
        },
        {
          "_id": "699d1f054e37ec6dfa1bc5fb",
          "name": "Peizhou Huang",
          "hidden": false
        },
        {
          "_id": "699d1f054e37ec6dfa1bc5fc",
          "name": "Mi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-23T14:37:01.000Z",
      "submittedOnDailyAt": "2026-02-24T01:16:59.044Z",
      "title": "DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning",
      "submittedOnDailyBy": {
        "_id": "640feb91b0ee289c8583dd59",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640feb91b0ee289c8583dd59/CA4UZj2mEsJO2UR0NnN5H.jpeg",
        "isPro": false,
        "fullname": "Hui Shen",
        "user": "Cloudriver",
        "type": "user"
      },
      "summary": "Reinforcement learning with verifiers (RLVR) is a central paradigm for improving large language model (LLM) reasoning, yet existing methods often suffer from limited exploration. Policies tend to collapse onto a few reasoning patterns and prematurely stop deep exploration, while conventional entropy regularization introduces only local stochasticity and fails to induce meaningful path-level diversity, leading to weak and unstable learning signals in group-based policy optimization. We propose DSDR, a Dual-Scale Diversity Regularization reinforcement learning framework that decomposes diversity in LLM reasoning into global and coupling components. Globally, DSDR promotes diversity among correct reasoning trajectories to explore distinct solution modes. Locally, it applies a length-invariant, token-level entropy regularization restricted to correct trajectories, preventing entropy collapse within each mode while preserving correctness. The two scales are coupled through a global-to-local allocation mechanism that emphasizes local regularization for more distinctive correct trajectories. We provide theoretical support showing that DSDR preserves optimal correctness under bounded regularization, sustains informative learning signals in group-based optimization, and yields a principled global-to-local coupling rule. Experiments on multiple reasoning benchmarks demonstrate consistent improvements in accuracy and pass@k, highlighting the importance of dual-scale diversity for deep exploration in RLVR. Code is available at https://github.com/SUSTechBruce/DSDR.",
      "upvotes": 9,
      "discussionId": "699d1f054e37ec6dfa1bc5fd",
      "ai_summary": "DSDR is a reinforcement learning framework that enhances large language model reasoning by promoting diversity at both global and local levels through dual-scale regularization techniques.",
      "ai_keywords": [
        "reinforcement learning",
        "large language models",
        "reasoning",
        "policy optimization",
        "entropy regularization",
        "diversity regularization",
        "global diversity",
        "local diversity",
        "token-level entropy",
        "trajectory diversity",
        "group-based optimization",
        "pass@k"
      ]
    },
    "publishedAt": "2026-02-23T09:37:01.000Z",
    "title": "DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning",
    "summary": "Reinforcement learning with verifiers (RLVR) is a central paradigm for improving large language model (LLM) reasoning, yet existing methods often suffer from limited exploration. Policies tend to collapse onto a few reasoning patterns and prematurely stop deep exploration, while conventional entropy regularization introduces only local stochasticity and fails to induce meaningful path-level diversity, leading to weak and unstable learning signals in group-based policy optimization. We propose DSDR, a Dual-Scale Diversity Regularization reinforcement learning framework that decomposes diversity in LLM reasoning into global and coupling components. Globally, DSDR promotes diversity among correct reasoning trajectories to explore distinct solution modes. Locally, it applies a length-invariant, token-level entropy regularization restricted to correct trajectories, preventing entropy collapse within each mode while preserving correctness. The two scales are coupled through a global-to-local allocation mechanism that emphasizes local regularization for more distinctive correct trajectories. We provide theoretical support showing that DSDR preserves optimal correctness under bounded regularization, sustains informative learning signals in group-based optimization, and yields a principled global-to-local coupling rule. Experiments on multiple reasoning benchmarks demonstrate consistent improvements in accuracy and pass@k, highlighting the importance of dual-scale diversity for deep exploration in RLVR. Code is available at https://github.com/SUSTechBruce/DSDR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.19895.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640feb91b0ee289c8583dd59",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640feb91b0ee289c8583dd59/CA4UZj2mEsJO2UR0NnN5H.jpeg",
      "fullname": "Hui Shen",
      "name": "Cloudriver",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.20021",
      "authors": [
        {
          "_id": "699d1e434e37ec6dfa1bc58f",
          "name": "Natalie Shapira",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc590",
          "name": "Chris Wendler",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc591",
          "name": "Avery Yen",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc592",
          "name": "Gabriele Sarti",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc593",
          "name": "Koyena Pal",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc594",
          "name": "Olivia Floody",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc595",
          "name": "Adam Belfki",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc596",
          "name": "Alex Loftus",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc597",
          "name": "Aditya Ratan Jannali",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc598",
          "name": "Nikhil Prakash",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc599",
          "name": "Jasmine Cui",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc59a",
          "name": "Giordano Rogers",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc59b",
          "name": "Jannik Brinkmann",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc59c",
          "name": "Can Rager",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc59d",
          "name": "Amir Zur",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc59e",
          "name": "Michael Ripa",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc59f",
          "name": "Aruna Sankaranarayanan",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc5a0",
          "name": "David Atkinson",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc5a1",
          "name": "Rohit Gandikota",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc5a2",
          "name": "Jaden Fiotto-Kaufman",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc5a3",
          "name": "EunJeong Hwang",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc5a4",
          "name": "Hadas Orgad",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc5a5",
          "name": "P Sam Sahil",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc5a6",
          "name": "Negev Taglicht",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc5a7",
          "name": "Tomer Shabtay",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc5a8",
          "name": "Atai Ambus",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc5a9",
          "name": "Nitay Alon",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc5aa",
          "name": "Shiri Oron",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc5ab",
          "name": "Ayelet Gordon-Tapiero",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc5ac",
          "name": "Yotam Kaplan",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc5ad",
          "name": "Vered Shwartz",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc5ae",
          "name": "Tamar Rott Shaham",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc5af",
          "name": "Christoph Riedl",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc5b0",
          "name": "Reuth Mirsky",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc5b1",
          "name": "Maarten Sap",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc5b2",
          "name": "David Manheim",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc5b3",
          "name": "Tomer Ullman",
          "hidden": false
        },
        {
          "_id": "699d1e434e37ec6dfa1bc5b4",
          "name": "David Bau",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-23T16:28:48.000Z",
      "submittedOnDailyAt": "2026-02-24T01:13:02.595Z",
      "title": "Agents of Chaos",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We report an exploratory red-teaming study of autonomous language-model-powered agents deployed in a live laboratory environment with persistent memory, email accounts, Discord access, file systems, and shell execution. Over a two-week period, twenty AI researchers interacted with the agents under benign and adversarial conditions. Focusing on failures emerging from the integration of language models with autonomy, tool use, and multi-party communication, we document eleven representative case studies. Observed behaviors include unauthorized compliance with non-owners, disclosure of sensitive information, execution of destructive system-level actions, denial-of-service conditions, uncontrolled resource consumption, identity spoofing vulnerabilities, cross-agent propagation of unsafe practices, and partial system takeover. In several cases, agents reported task completion while the underlying system state contradicted those reports. We also report on some of the failed attempts. Our findings establish the existence of security-, privacy-, and governance-relevant vulnerabilities in realistic deployment settings. These behaviors raise unresolved questions regarding accountability, delegated authority, and responsibility for downstream harms, and warrant urgent attention from legal scholars, policymakers, and researchers across disciplines. This report serves as an initial empirical contribution to that broader conversation.",
      "upvotes": 6,
      "discussionId": "699d1e434e37ec6dfa1bc5b5",
      "projectPage": "https://agentsofchaos.baulab.info/",
      "ai_summary": "Autonomous language-model-powered agents in a live laboratory environment exhibited numerous security and governance vulnerabilities including unauthorized actions, information disclosure, and system takeovers during a two-week study with twenty researchers.",
      "ai_keywords": [
        "red-teaming study",
        "autonomous language-model-powered agents",
        "persistent memory",
        "tool use",
        "multi-party communication",
        "security vulnerabilities",
        "privacy vulnerabilities",
        "governance vulnerabilities",
        "accountability",
        "delegated authority",
        "responsibility"
      ]
    },
    "publishedAt": "2026-02-23T11:28:48.000Z",
    "title": "Agents of Chaos",
    "summary": "We report an exploratory red-teaming study of autonomous language-model-powered agents deployed in a live laboratory environment with persistent memory, email accounts, Discord access, file systems, and shell execution. Over a two-week period, twenty AI researchers interacted with the agents under benign and adversarial conditions. Focusing on failures emerging from the integration of language models with autonomy, tool use, and multi-party communication, we document eleven representative case studies. Observed behaviors include unauthorized compliance with non-owners, disclosure of sensitive information, execution of destructive system-level actions, denial-of-service conditions, uncontrolled resource consumption, identity spoofing vulnerabilities, cross-agent propagation of unsafe practices, and partial system takeover. In several cases, agents reported task completion while the underlying system state contradicted those reports. We also report on some of the failed attempts. Our findings establish the existence of security-, privacy-, and governance-relevant vulnerabilities in realistic deployment settings. These behaviors raise unresolved questions regarding accountability, delegated authority, and responsibility for downstream harms, and warrant urgent attention from legal scholars, policymakers, and researchers across disciplines. This report serves as an initial empirical contribution to that broader conversation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20021.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 237,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.19672",
      "authors": [
        {
          "_id": "699d407c4e37ec6dfa1bc6a7",
          "name": "Jiayu Wang",
          "hidden": false
        },
        {
          "_id": "699d407c4e37ec6dfa1bc6a8",
          "name": "Yifei Ming",
          "hidden": false
        },
        {
          "_id": "699d407c4e37ec6dfa1bc6a9",
          "name": "Zixuan Ke",
          "hidden": false
        },
        {
          "_id": "699d407c4e37ec6dfa1bc6aa",
          "name": "Shafiq Joty",
          "hidden": false
        },
        {
          "_id": "699d407c4e37ec6dfa1bc6ab",
          "name": "Aws Albarghouthi",
          "hidden": false
        },
        {
          "_id": "699d407c4e37ec6dfa1bc6ac",
          "name": "Frederic Sala",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-23T10:17:25.000Z",
      "submittedOnDailyAt": "2026-02-24T03:44:00.879Z",
      "title": "SkillOrchestra: Learning to Route Agents via Skill Transfer",
      "submittedOnDailyBy": {
        "_id": "651651f5d93a51ceda3021c3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651651f5d93a51ceda3021c3/FE2uGpTKBRWMKTDBv1H-g.png",
        "isPro": false,
        "fullname": "Jiayu (Mila) Wang",
        "user": "MilaWang",
        "type": "user"
      },
      "summary": "Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration. Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse, repeatedly invoking one strong but costly option in multi-turn scenarios. We introduce SkillOrchestra, a framework for skill-aware orchestration. Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off. Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration, offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra.",
      "upvotes": 4,
      "discussionId": "699d407c4e37ec6dfa1bc6ad",
      "githubRepo": "https://github.com/jiayuww/SkillOrchestra",
      "githubRepoAddedBy": "user",
      "ai_summary": "SkillOrchestra presents a skill-aware orchestration framework that improves compound AI system performance through fine-grained skill modeling and efficient agent selection, achieving superior results with significantly reduced learning costs compared to reinforcement learning-based methods.",
      "ai_keywords": [
        "compound AI systems",
        "orchestration",
        "routing policy",
        "reinforcement learning",
        "skill modeling",
        "agent-specific competence",
        "performance-cost trade-off",
        "multi-turn scenarios",
        "routing collapse",
        "end-to-end learning"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "61d090ec03bc10eb8e1c2970",
        "name": "uw-madison",
        "fullname": "University of Wisconsin - Madison",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/IYmUaLUc_rDVNC6F7-k8M.png"
      }
    },
    "publishedAt": "2026-02-23T05:17:25.000Z",
    "title": "SkillOrchestra: Learning to Route Agents via Skill Transfer",
    "summary": "Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration. Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse, repeatedly invoking one strong but costly option in multi-turn scenarios. We introduce SkillOrchestra, a framework for skill-aware orchestration. Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off. Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration, offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.19672.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "651651f5d93a51ceda3021c3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651651f5d93a51ceda3021c3/FE2uGpTKBRWMKTDBv1H-g.png",
      "fullname": "Jiayu (Mila) Wang",
      "name": "MilaWang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "61d090ec03bc10eb8e1c2970",
      "name": "uw-madison",
      "fullname": "University of Wisconsin - Madison",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/IYmUaLUc_rDVNC6F7-k8M.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.19128",
      "authors": [
        {
          "_id": "699d21fb4e37ec6dfa1bc621",
          "name": "Shiyi Cao",
          "hidden": false
        },
        {
          "_id": "699d21fb4e37ec6dfa1bc622",
          "name": "Ziming Mao",
          "hidden": false
        },
        {
          "_id": "699d21fb4e37ec6dfa1bc623",
          "name": "Joseph E. Gonzalez",
          "hidden": false
        },
        {
          "_id": "699d21fb4e37ec6dfa1bc624",
          "name": "Ion Stoica",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-22T11:06:22.000Z",
      "submittedOnDailyAt": "2026-02-24T01:29:01.376Z",
      "title": "K-Search: LLM Kernel Generation via Co-Evolving Intrinsic World Model",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Optimizing GPU kernels is critical for efficient modern machine learning systems yet remains challenging due to the complex interplay of design factors and rapid hardware evolution. Existing automated approaches typically treat Large Language Models (LLMs) merely as stochastic code generators within heuristic-guided evolutionary loops. These methods often struggle with complex kernels requiring coordinated, multi-step structural transformations, as they lack explicit planning capabilities and frequently discard promising strategies due to inefficient or incorrect intermediate implementations. To address this, we propose Search via Co-Evolving World Model and build K-Search based on this method. By replacing static search heuristics with a co-evolving world model, our framework leverages LLMs' prior domain knowledge to guide the search, actively exploring the optimization space. This approach explicitly decouples high-level algorithmic planning from low-level program instantiation, enabling the system to navigate non-monotonic optimization paths while remaining resilient to temporary implementation defects. We evaluate K-Search on diverse, complex kernels from FlashInfer, including GQA, MLA, and MoE kernels. Our results show that K-Search significantly outperforms state-of-the-art evolutionary search methods, achieving an average 2.10x improvement and up to a 14.3x gain on complex MoE kernels. On the GPUMode TriMul task, K-Search achieves state-of-the-art performance on H100, reaching 1030us and surpassing both prior evolution and human-designed solutions.",
      "upvotes": 3,
      "discussionId": "699d21fc4e37ec6dfa1bc625",
      "ai_summary": "K-Search uses a co-evolving world model to optimize GPU kernels by separating high-level planning from low-level implementation, achieving significant performance improvements over existing evolutionary methods.",
      "ai_keywords": [
        "GPU kernels",
        "Large Language Models",
        "evolutionary search",
        "world model",
        "co-evolution",
        "algorithmic planning",
        "program instantiation",
        "non-monotonic optimization",
        "FlashInfer",
        "GQA",
        "MLA",
        "MoE kernels",
        "GPUMode TriMul"
      ]
    },
    "publishedAt": "2026-02-22T06:06:22.000Z",
    "title": "K-Search: LLM Kernel Generation via Co-Evolving Intrinsic World Model",
    "summary": "Optimizing GPU kernels is critical for efficient modern machine learning systems yet remains challenging due to the complex interplay of design factors and rapid hardware evolution. Existing automated approaches typically treat Large Language Models (LLMs) merely as stochastic code generators within heuristic-guided evolutionary loops. These methods often struggle with complex kernels requiring coordinated, multi-step structural transformations, as they lack explicit planning capabilities and frequently discard promising strategies due to inefficient or incorrect intermediate implementations. To address this, we propose Search via Co-Evolving World Model and build K-Search based on this method. By replacing static search heuristics with a co-evolving world model, our framework leverages LLMs' prior domain knowledge to guide the search, actively exploring the optimization space. This approach explicitly decouples high-level algorithmic planning from low-level program instantiation, enabling the system to navigate non-monotonic optimization paths while remaining resilient to temporary implementation defects. We evaluate K-Search on diverse, complex kernels from FlashInfer, including GQA, MLA, and MoE kernels. Our results show that K-Search significantly outperforms state-of-the-art evolutionary search methods, achieving an average 2.10x improvement and up to a 14.3x gain on complex MoE kernels. On the GPUMode TriMul task, K-Search achieves state-of-the-art performance on H100, reaching 1030us and surpassing both prior evolution and human-designed solutions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.19128.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 237,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.20160",
      "authors": [
        {
          "_id": "699d14f34e37ec6dfa1bc510",
          "name": "Chen Wang",
          "hidden": false
        },
        {
          "_id": "699d14f34e37ec6dfa1bc511",
          "name": "Hao Tan",
          "hidden": false
        },
        {
          "_id": "699d14f34e37ec6dfa1bc512",
          "name": "Wang Yifan",
          "hidden": false
        },
        {
          "_id": "699d14f34e37ec6dfa1bc513",
          "name": "Zhiqin Chen",
          "hidden": false
        },
        {
          "_id": "699d14f34e37ec6dfa1bc514",
          "name": "Yuheng Liu",
          "hidden": false
        },
        {
          "_id": "699d14f34e37ec6dfa1bc515",
          "name": "Kalyan Sunkavalli",
          "hidden": false
        },
        {
          "_id": "699d14f34e37ec6dfa1bc516",
          "name": "Sai Bi",
          "hidden": false
        },
        {
          "_id": "699d14f34e37ec6dfa1bc517",
          "name": "Lingjie Liu",
          "hidden": false
        },
        {
          "_id": "699d14f34e37ec6dfa1bc518",
          "name": "Yiwei Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-23T18:59:45.000Z",
      "submittedOnDailyAt": "2026-02-24T00:35:04.916Z",
      "title": "tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction",
      "submittedOnDailyBy": {
        "_id": "62e9d5af6687b60b9c01240a",
        "avatarUrl": "/avatars/3983be5662aa28aefcb18deaf08d7cb1.svg",
        "isPro": true,
        "fullname": "Chen Wang",
        "user": "chenwang",
        "type": "user"
      },
      "summary": "We propose tttLRM, a novel large 3D reconstruction model that leverages a Test-Time Training (TTT) layer to enable long-context, autoregressive 3D reconstruction with linear computational complexity, further scaling the model's capability. Our framework efficiently compresses multiple image observations into the fast weights of the TTT layer, forming an implicit 3D representation in the latent space that can be decoded into various explicit formats, such as Gaussian Splats (GS) for downstream applications. The online learning variant of our model supports progressive 3D reconstruction and refinement from streaming observations. We demonstrate that pretraining on novel view synthesis tasks effectively transfers to explicit 3D modeling, resulting in improved reconstruction quality and faster convergence. Extensive experiments show that our method achieves superior performance in feedforward 3D Gaussian reconstruction compared to state-of-the-art approaches on both objects and scenes.",
      "upvotes": 1,
      "discussionId": "699d14f34e37ec6dfa1bc519",
      "projectPage": "https://cwchenwang.github.io/tttLRM",
      "githubRepo": "https://github.com/cwchenwang/tttLRM",
      "githubRepoAddedBy": "user",
      "ai_summary": "A novel 3D reconstruction model called tttLRM uses a Test-Time Training layer to enable efficient, scalable autoregressive reconstruction with linear complexity, achieving better results than existing methods.",
      "ai_keywords": [
        "Test-Time Training",
        "autoregressive 3D reconstruction",
        "fast weights",
        "latent space",
        "Gaussian Splats",
        "progressive reconstruction",
        "novel view synthesis",
        "explicit 3D modeling",
        "feedforward 3D Gaussian reconstruction"
      ],
      "githubStars": 28
    },
    "publishedAt": "2026-02-23T13:59:45.000Z",
    "title": "tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction",
    "summary": "We propose tttLRM, a novel large 3D reconstruction model that leverages a Test-Time Training (TTT) layer to enable long-context, autoregressive 3D reconstruction with linear computational complexity, further scaling the model's capability. Our framework efficiently compresses multiple image observations into the fast weights of the TTT layer, forming an implicit 3D representation in the latent space that can be decoded into various explicit formats, such as Gaussian Splats (GS) for downstream applications. The online learning variant of our model supports progressive 3D reconstruction and refinement from streaming observations. We demonstrate that pretraining on novel view synthesis tasks effectively transfers to explicit 3D modeling, resulting in improved reconstruction quality and faster convergence. Extensive experiments show that our method achieves superior performance in feedforward 3D Gaussian reconstruction compared to state-of-the-art approaches on both objects and scenes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20160.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e9d5af6687b60b9c01240a",
      "avatarUrl": "/avatars/3983be5662aa28aefcb18deaf08d7cb1.svg",
      "fullname": "Chen Wang",
      "name": "chenwang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.18915",
      "authors": [
        {
          "_id": "699d14764e37ec6dfa1bc4ff",
          "name": "Mohammadreza Ghaffarzadeh-Esfahani",
          "hidden": false
        },
        {
          "_id": "699d14764e37ec6dfa1bc500",
          "name": "Yousof Gheisari",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/645d63c0ce72244df7b36be8/0GpwlhIgxtkJTNTlQbMNY.png"
      ],
      "publishedAt": "2026-02-21T17:46:34.000Z",
      "submittedOnDailyAt": "2026-02-24T02:01:32.951Z",
      "title": "AAVGen: Precision Engineering of Adeno-associated Viral Capsids for Renal Selective Targeting",
      "submittedOnDailyBy": {
        "_id": "645d63c0ce72244df7b36be8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645d63c0ce72244df7b36be8/09vhYAzgv1svwvQM4eIE9.jpeg",
        "isPro": false,
        "fullname": "MoRezaGH",
        "user": "Moreza009",
        "type": "user"
      },
      "summary": "Adeno-associated viruses (AAVs) are promising vectors for gene therapy, but their native serotypes face limitations in tissue tropism, immune evasion, and production efficiency. Engineering capsids to overcome these hurdles is challenging due to the vast sequence space and the difficulty of simultaneously optimizing multiple functional properties. The complexity also adds when it comes to the kidney, which presents unique anatomical barriers and cellular targets that require precise and efficient vector engineering. Here, we present AAVGen, a generative artificial intelligence framework for de novo design of AAV capsids with enhanced multi-trait profiles. AAVGen integrates a protein language model (PLM) with supervised fine-tuning (SFT) and a reinforcement learning technique termed Group Sequence Policy Optimization (GSPO). The model is guided by a composite reward signal derived from three ESM-2-based regression predictors, each trained to predict a key property: production fitness, kidney tropism, and thermostability. Our results demonstrate that AAVGen produces a diverse library of novel VP1 protein sequences. In silico validations revealed that the majority of the generated variants have superior performance across all three employed indices, indicating successful multi-objective optimization. Furthermore, structural analysis via AlphaFold3 confirms that the generated sequences preserve the canonical capsid folding despite sequence diversification. AAVGen establishes a foundation for data-driven viral vector engineering, accelerating the development of next-generation AAV vectors with tailored functional characteristics.",
      "upvotes": 1,
      "discussionId": "699d14764e37ec6dfa1bc501",
      "githubRepo": "https://github.com/mohammad-gh009/AAVGen",
      "githubRepoAddedBy": "user",
      "ai_summary": "AAVGen is a generative AI framework that designs AAV capsids with improved traits through protein language models, supervised fine-tuning, and reinforcement learning techniques.",
      "ai_keywords": [
        "protein language model",
        "supervised fine-tuning",
        "reinforcement learning",
        "Group Sequence Policy Optimization",
        "ESM-2-based regression predictors",
        "multi-objective optimization",
        "AlphaFold3"
      ],
      "githubStars": 0
    },
    "publishedAt": "2026-02-21T12:46:34.000Z",
    "title": "AAVGen: Precision Engineering of Adeno-associated Viral Capsids for Renal Selective Targeting",
    "summary": "Adeno-associated viruses (AAVs) are promising vectors for gene therapy, but their native serotypes face limitations in tissue tropism, immune evasion, and production efficiency. Engineering capsids to overcome these hurdles is challenging due to the vast sequence space and the difficulty of simultaneously optimizing multiple functional properties. The complexity also adds when it comes to the kidney, which presents unique anatomical barriers and cellular targets that require precise and efficient vector engineering. Here, we present AAVGen, a generative artificial intelligence framework for de novo design of AAV capsids with enhanced multi-trait profiles. AAVGen integrates a protein language model (PLM) with supervised fine-tuning (SFT) and a reinforcement learning technique termed Group Sequence Policy Optimization (GSPO). The model is guided by a composite reward signal derived from three ESM-2-based regression predictors, each trained to predict a key property: production fitness, kidney tropism, and thermostability. Our results demonstrate that AAVGen produces a diverse library of novel VP1 protein sequences. In silico validations revealed that the majority of the generated variants have superior performance across all three employed indices, indicating successful multi-objective optimization. Furthermore, structural analysis via AlphaFold3 confirms that the generated sequences preserve the canonical capsid folding despite sequence diversification. AAVGen establishes a foundation for data-driven viral vector engineering, accelerating the development of next-generation AAV vectors with tailored functional characteristics.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645d63c0ce72244df7b36be8/0GpwlhIgxtkJTNTlQbMNY.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18915.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645d63c0ce72244df7b36be8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645d63c0ce72244df7b36be8/09vhYAzgv1svwvQM4eIE9.jpeg",
      "fullname": "MoRezaGH",
      "name": "Moreza009",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.12100",
      "authors": [
        {
          "_id": "699d4d754e37ec6dfa1bc6c8",
          "name": "Lingting Zhu",
          "hidden": false
        },
        {
          "_id": "699d4d754e37ec6dfa1bc6c9",
          "name": "Shengju Qian",
          "hidden": false
        },
        {
          "_id": "699d4d754e37ec6dfa1bc6ca",
          "name": "Haidi Fan",
          "hidden": false
        },
        {
          "_id": "699d4d754e37ec6dfa1bc6cb",
          "name": "Jiayu Dong",
          "hidden": false
        },
        {
          "_id": "699d4d754e37ec6dfa1bc6cc",
          "name": "Zhenchao Jin",
          "hidden": false
        },
        {
          "_id": "699d4d754e37ec6dfa1bc6cd",
          "name": "Siwei Zhou",
          "hidden": false
        },
        {
          "_id": "699d4d754e37ec6dfa1bc6ce",
          "name": "Gen Dong",
          "hidden": false
        },
        {
          "_id": "699d4d754e37ec6dfa1bc6cf",
          "name": "Xin Wang",
          "hidden": false
        },
        {
          "_id": "699d4d754e37ec6dfa1bc6d0",
          "name": "Lequan Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-12T15:55:21.000Z",
      "submittedOnDailyAt": "2026-02-24T04:36:24.875Z",
      "title": "AssetFormer: Modular 3D Assets Generation with Autoregressive Transformer",
      "submittedOnDailyBy": {
        "_id": "6423f5e6774cc34079730f31",
        "avatarUrl": "/avatars/7ebeb1f623c86b1a676c95bb67572f8b.svg",
        "isPro": false,
        "fullname": "Lingting Zhu",
        "user": "ltzhu",
        "type": "user"
      },
      "summary": "The digital industry demands high-quality, diverse modular 3D assets, especially for user-generated content~(UGC). In this work, we introduce AssetFormer, an autoregressive Transformer-based model designed to generate modular 3D assets from textual descriptions. Our pilot study leverages real-world modular assets collected from online platforms. AssetFormer tackles the challenge of creating assets composed of primitives that adhere to constrained design parameters for various applications. By innovatively adapting module sequencing and decoding techniques inspired by language models, our approach enhances asset generation quality through autoregressive modeling. Initial results indicate the effectiveness of AssetFormer in streamlining asset creation for professional development and UGC scenarios. This work presents a flexible framework extendable to various types of modular 3D assets, contributing to the broader field of 3D content generation. The code is available at https://github.com/Advocate99/AssetFormer.",
      "upvotes": 1,
      "discussionId": "699d4d764e37ec6dfa1bc6d1",
      "githubRepo": "https://github.com/Advocate99/AssetFormer",
      "githubRepoAddedBy": "user",
      "ai_summary": "AssetFormer is an autoregressive Transformer-based model that generates modular 3D assets from textual descriptions by adapting language model techniques to handle design constraints.",
      "ai_keywords": [
        "Transformer-based model",
        "autoregressive modeling",
        "modular 3D assets",
        "textual descriptions",
        "module sequencing",
        "decoding techniques"
      ],
      "githubStars": 24,
      "organization": {
        "_id": "67ea9ecfc234715db8dbf339",
        "name": "hkuhk",
        "fullname": "The University of Hong Kong",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
      }
    },
    "publishedAt": "2026-02-12T10:55:21.000Z",
    "title": "AssetFormer: Modular 3D Assets Generation with Autoregressive Transformer",
    "summary": "The digital industry demands high-quality, diverse modular 3D assets, especially for user-generated content~(UGC). In this work, we introduce AssetFormer, an autoregressive Transformer-based model designed to generate modular 3D assets from textual descriptions. Our pilot study leverages real-world modular assets collected from online platforms. AssetFormer tackles the challenge of creating assets composed of primitives that adhere to constrained design parameters for various applications. By innovatively adapting module sequencing and decoding techniques inspired by language models, our approach enhances asset generation quality through autoregressive modeling. Initial results indicate the effectiveness of AssetFormer in streamlining asset creation for professional development and UGC scenarios. This work presents a flexible framework extendable to various types of modular 3D assets, contributing to the broader field of 3D content generation. The code is available at https://github.com/Advocate99/AssetFormer.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12100.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6423f5e6774cc34079730f31",
      "avatarUrl": "/avatars/7ebeb1f623c86b1a676c95bb67572f8b.svg",
      "fullname": "Lingting Zhu",
      "name": "ltzhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "67ea9ecfc234715db8dbf339",
      "name": "hkuhk",
      "fullname": "The University of Hong Kong",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.19455",
      "authors": [
        {
          "_id": "699d21974e37ec6dfa1bc612",
          "name": "Zelin He",
          "hidden": false
        },
        {
          "_id": "699d21974e37ec6dfa1bc613",
          "name": "Boran Han",
          "hidden": false
        },
        {
          "_id": "699d21974e37ec6dfa1bc614",
          "name": "Xiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "699d21974e37ec6dfa1bc615",
          "name": "Shuai Zhang",
          "hidden": false
        },
        {
          "_id": "699d21974e37ec6dfa1bc616",
          "name": "Haotian Lin",
          "hidden": false
        },
        {
          "_id": "699d21974e37ec6dfa1bc617",
          "name": "Qi Zhu",
          "hidden": false
        },
        {
          "_id": "699d21974e37ec6dfa1bc618",
          "name": "Haoyang Fang",
          "hidden": false
        },
        {
          "_id": "699d21974e37ec6dfa1bc619",
          "name": "Danielle C. Maddix",
          "hidden": false
        },
        {
          "_id": "699d21974e37ec6dfa1bc61a",
          "name": "Abdul Fatir Ansari",
          "hidden": false
        },
        {
          "_id": "699d21974e37ec6dfa1bc61b",
          "name": "Akash Chandrayan",
          "hidden": false
        },
        {
          "_id": "699d21974e37ec6dfa1bc61c",
          "name": "Abhinav Pradhan",
          "hidden": false
        },
        {
          "_id": "699d21974e37ec6dfa1bc61d",
          "name": "Bernie Wang",
          "hidden": false
        },
        {
          "_id": "699d21974e37ec6dfa1bc61e",
          "name": "Matthew Reimherr",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-23T02:55:32.000Z",
      "submittedOnDailyAt": "2026-02-24T01:27:18.878Z",
      "title": "SenTSR-Bench: Thinking with Injected Knowledge for Time-Series Reasoning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Time-series diagnostic reasoning is essential for many applications, yet existing solutions face a persistent gap: general reasoning large language models (GRLMs) possess strong reasoning skills but lack the domain-specific knowledge to understand complex time-series patterns. Conversely, fine-tuned time-series LLMs (TSLMs) understand these patterns but lack the capacity to generalize reasoning for more complicated questions. To bridge this gap, we propose a hybrid knowledge-injection framework that injects TSLM-generated insights directly into GRLM's reasoning trace, thereby achieving strong time-series reasoning with in-domain knowledge. As collecting data for knowledge injection fine-tuning is costly, we further leverage a reinforcement learning-based approach with verifiable rewards (RLVR) to elicit knowledge-rich traces without human supervision, then transfer such an in-domain thinking trace into GRLM for efficient knowledge injection. We further release SenTSR-Bench, a multivariate time-series-based diagnostic reasoning benchmark collected from real-world industrial operations. Across SenTSR-Bench and other public datasets, our method consistently surpasses TSLMs by 9.1%-26.1% and GRLMs by 7.9%-22.4%, delivering robust, context-aware time-series diagnostic insights.",
      "upvotes": 0,
      "discussionId": "699d21984e37ec6dfa1bc61f",
      "ai_summary": "A hybrid knowledge-injection framework combines general reasoning large language models with time-series LLMs through reinforcement learning-based verifiable rewards to enhance time-series diagnostic reasoning performance.",
      "ai_keywords": [
        "large language models",
        "time-series LLMs",
        "knowledge injection",
        "reinforcement learning",
        "verifiable rewards",
        "diagnostic reasoning",
        "multivariate time-series",
        "benchmark"
      ]
    },
    "publishedAt": "2026-02-22T21:55:32.000Z",
    "title": "SenTSR-Bench: Thinking with Injected Knowledge for Time-Series Reasoning",
    "summary": "Time-series diagnostic reasoning is essential for many applications, yet existing solutions face a persistent gap: general reasoning large language models (GRLMs) possess strong reasoning skills but lack the domain-specific knowledge to understand complex time-series patterns. Conversely, fine-tuned time-series LLMs (TSLMs) understand these patterns but lack the capacity to generalize reasoning for more complicated questions. To bridge this gap, we propose a hybrid knowledge-injection framework that injects TSLM-generated insights directly into GRLM's reasoning trace, thereby achieving strong time-series reasoning with in-domain knowledge. As collecting data for knowledge injection fine-tuning is costly, we further leverage a reinforcement learning-based approach with verifiable rewards (RLVR) to elicit knowledge-rich traces without human supervision, then transfer such an in-domain thinking trace into GRLM for efficient knowledge injection. We further release SenTSR-Bench, a multivariate time-series-based diagnostic reasoning benchmark collected from real-world industrial operations. Across SenTSR-Bench and other public datasets, our method consistently surpasses TSLMs by 9.1%-26.1% and GRLMs by 7.9%-22.4%, delivering robust, context-aware time-series diagnostic insights.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.19455.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 237,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  }
]