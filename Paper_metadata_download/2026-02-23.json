[
  {
    "paper": {
      "id": "2602.10693",
      "authors": [
        {
          "_id": "6992047b50fb2c0be47837f0",
          "user": {
            "_id": "6475ff9b4c9fb8a4bf1cde76",
            "avatarUrl": "/avatars/61cf82cd0e15c4618f5bd8b1f7d52f37.svg",
            "isPro": false,
            "fullname": "floyed shen",
            "user": "floyed",
            "type": "user"
          },
          "name": "Guobin Shen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-17T15:52:51.206Z",
          "hidden": false
        },
        {
          "_id": "6992047b50fb2c0be47837f1",
          "user": {
            "_id": "63fc5b724c57549ad5e54558",
            "avatarUrl": "/avatars/1374c1e8969533dd7543959666f16d1a.svg",
            "isPro": false,
            "fullname": "Chenxiao Zhao",
            "user": "ChenShawn",
            "type": "user"
          },
          "name": "Chenxiao Zhao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-02-17T17:17:12.583Z",
          "hidden": false
        },
        {
          "_id": "6992047b50fb2c0be47837f2",
          "user": {
            "_id": "655c43d6b426ec8f4b5e7652",
            "avatarUrl": "/avatars/ddcf9d1ef0e2dc1f564a56ba9153f24f.svg",
            "isPro": false,
            "fullname": "Xiang Cheng",
            "user": "FFFc2",
            "type": "user"
          },
          "name": "Xiang Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-17T15:52:57.697Z",
          "hidden": false
        },
        {
          "_id": "6992047b50fb2c0be47837f3",
          "name": "Lei Huang",
          "hidden": false
        },
        {
          "_id": "6992047b50fb2c0be47837f4",
          "name": "Xing Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-11T09:48:08.000Z",
      "submittedOnDailyAt": "2026-02-23T03:29:14.259Z",
      "title": "VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training",
      "submittedOnDailyBy": {
        "_id": "6475ff9b4c9fb8a4bf1cde76",
        "avatarUrl": "/avatars/61cf82cd0e15c4618f5bd8b1f7d52f37.svg",
        "isPro": false,
        "fullname": "floyed shen",
        "user": "floyed",
        "type": "user"
      },
      "summary": "Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO",
      "upvotes": 149,
      "discussionId": "6992047c50fb2c0be47837f5",
      "githubRepo": "https://github.com/FloyedShen/VESPO",
      "githubRepoAddedBy": "user",
      "ai_summary": "VESPO addresses training instability in LLM reinforcement learning by using variational formulation with variance reduction to correct policy divergence without length normalization.",
      "ai_keywords": [
        "reinforcement learning",
        "large language models",
        "policy staleness",
        "asynchronous training",
        "importance sampling",
        "variance reduction",
        "variational formulation",
        "proposal distributions",
        "sequence-level importance weights",
        "mathematical reasoning benchmarks",
        "Mixture-of-Experts models"
      ],
      "githubStars": 9,
      "organization": {
        "_id": "68246a0a98117c02df67a547",
        "name": "rednote-hilab",
        "fullname": "rednote-hilab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6807a1d6504547b3554b9c73/WgnnQDsz7FqnyTtv8mmRO.png"
      }
    },
    "publishedAt": "2026-02-11T04:48:08.000Z",
    "title": "VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training",
    "summary": "Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10693.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6475ff9b4c9fb8a4bf1cde76",
      "avatarUrl": "/avatars/61cf82cd0e15c4618f5bd8b1f7d52f37.svg",
      "fullname": "floyed shen",
      "name": "floyed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "68246a0a98117c02df67a547",
      "name": "rednote-hilab",
      "fullname": "rednote-hilab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6807a1d6504547b3554b9c73/WgnnQDsz7FqnyTtv8mmRO.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.08354",
      "authors": [
        {
          "_id": "699bcb43f723198bd53a633e",
          "name": "Zixuan Huang",
          "hidden": false
        },
        {
          "_id": "699bcb43f723198bd53a633f",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "699bcb43f723198bd53a6340",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "699bcb43f723198bd53a6341",
          "name": "Jianbin Zheng",
          "hidden": false
        },
        {
          "_id": "699bcb43f723198bd53a6342",
          "name": "Xuanda Wang",
          "hidden": false
        },
        {
          "_id": "699bcb43f723198bd53a6343",
          "name": "Zhixia Zhang",
          "hidden": false
        },
        {
          "_id": "699bcb43f723198bd53a6344",
          "name": "Hongyan Xie",
          "hidden": false
        },
        {
          "_id": "699bcb43f723198bd53a6345",
          "name": "Songshi Liang",
          "hidden": false
        },
        {
          "_id": "699bcb43f723198bd53a6346",
          "name": "Zehao Chen",
          "hidden": false
        },
        {
          "_id": "699bcb43f723198bd53a6347",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "699bcb43f723198bd53a6348",
          "name": "Fuzhen Zhuang",
          "hidden": false
        },
        {
          "_id": "699bcb43f723198bd53a6349",
          "name": "Jianxin Li",
          "hidden": false
        },
        {
          "_id": "699bcb43f723198bd53a634a",
          "name": "Yikun Ban",
          "hidden": false
        },
        {
          "_id": "699bcb43f723198bd53a634b",
          "name": "Deqing Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T07:38:22.000Z",
      "submittedOnDailyAt": "2026-02-23T01:06:54.391Z",
      "title": "Does Your Reasoning Model Implicitly Know When to Stop Thinking?",
      "submittedOnDailyBy": {
        "_id": "68345345f4bbf856e2d708e2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg",
        "isPro": false,
        "fullname": "Yikun B",
        "user": "Yikunb",
        "type": "user"
      },
      "summary": "Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.",
      "upvotes": 53,
      "discussionId": "699bcb44f723198bd53a634c",
      "projectPage": "https://hzx122.github.io/sage-rl/",
      "ai_summary": "Large reasoning models can implicitly determine optimal stopping points for thinking, which SAGE-RL enhances by incorporating efficient reasoning patterns into pass@1 inference for improved accuracy and efficiency.",
      "ai_keywords": [
        "large reasoning models",
        "chains of thought",
        "sampling paradigms",
        "self-aware guided efficient reasoning",
        "group-based reinforcement learning",
        "pass@1 inference",
        "mathematical benchmarks"
      ],
      "organization": {
        "_id": "653b817d32c97d0655575872",
        "name": "ByteDance",
        "fullname": "ByteDance",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
      }
    },
    "publishedAt": "2026-02-09T02:38:22.000Z",
    "title": "Does Your Reasoning Model Implicitly Know When to Stop Thinking?",
    "summary": "Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08354.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "68345345f4bbf856e2d708e2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg",
      "fullname": "Yikun B",
      "name": "Yikunb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "653b817d32c97d0655575872",
      "name": "ByteDance",
      "fullname": "ByteDance",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.18422",
      "authors": [
        {
          "_id": "699bbf5bf723198bd53a6309",
          "name": "Linxi Xie",
          "hidden": false
        },
        {
          "_id": "699bbf5bf723198bd53a630a",
          "name": "Lisong C. Sun",
          "hidden": false
        },
        {
          "_id": "699bbf5bf723198bd53a630b",
          "name": "Ashley Neall",
          "hidden": false
        },
        {
          "_id": "699bbf5bf723198bd53a630c",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "699bbf5bf723198bd53a630d",
          "name": "Shengqu Cai",
          "hidden": false
        },
        {
          "_id": "699bbf5bf723198bd53a630e",
          "name": "Gordon Wetzstein",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/6_AD51fcx9Fvk7_TGemIO.mp4"
      ],
      "publishedAt": "2026-02-20T18:45:29.000Z",
      "submittedOnDailyAt": "2026-02-23T00:17:53.289Z",
      "title": "Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.",
      "upvotes": 4,
      "discussionId": "699bbf5cf723198bd53a630f",
      "projectPage": "https://codeysun.github.io/generated-reality/",
      "ai_summary": "A human-centric video world model conditioned on tracked head and hand poses is introduced, enabling dexterous interactions through a bidirectional video diffusion model trained for egocentric virtual environment generation.",
      "ai_keywords": [
        "video world models",
        "diffusion transformer",
        "3D head pose",
        "joint-level hand poses",
        "dexterous hand-object interactions",
        "bidirectional video diffusion model",
        "causal interactive system",
        "egocentric virtual environments"
      ]
    },
    "publishedAt": "2026-02-20T13:45:29.000Z",
    "title": "Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control",
    "summary": "Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/6_AD51fcx9Fvk7_TGemIO.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18422.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 237,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.18071",
      "authors": [
        {
          "_id": "699bbee4f723198bd53a6300",
          "name": "Boyuan An",
          "hidden": false
        },
        {
          "_id": "699bbee4f723198bd53a6301",
          "name": "Zhexiong Wang",
          "hidden": false
        },
        {
          "_id": "699bbee4f723198bd53a6302",
          "name": "Yipeng Wang",
          "hidden": false
        },
        {
          "_id": "699bbee4f723198bd53a6303",
          "name": "Jiaqi Li",
          "hidden": false
        },
        {
          "_id": "699bbee4f723198bd53a6304",
          "name": "Sihang Li",
          "hidden": false
        },
        {
          "_id": "699bbee4f723198bd53a6305",
          "name": "Jing Zhang",
          "hidden": false
        },
        {
          "_id": "699bbee4f723198bd53a6306",
          "name": "Chen Feng",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/JZMXdQ95tWmAWkwvLCHIa.mp4"
      ],
      "publishedAt": "2026-02-20T08:54:20.000Z",
      "submittedOnDailyAt": "2026-02-23T00:14:04.117Z",
      "title": "EgoPush: Learning End-to-End Egocentric Multi-Object Rearrangement for Mobile Robots",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Humans can rearrange objects in cluttered environments using egocentric perception, navigating occlusions without global coordinates. Inspired by this capability, we study long-horizon multi-object non-prehensile rearrangement for mobile robots using a single egocentric camera. We introduce EgoPush, a policy learning framework that enables egocentric, perception-driven rearrangement without relying on explicit global state estimation that often fails in dynamic scenes. EgoPush designs an object-centric latent space to encode relative spatial relations among objects, rather than absolute poses. This design enables a privileged reinforcement-learning (RL) teacher to jointly learn latent states and mobile actions from sparse keypoints, which is then distilled into a purely visual student policy. To reduce the supervision gap between the omniscient teacher and the partially observed student, we restrict the teacher's observations to visually accessible cues. This induces active perception behaviors that are recoverable from the student's viewpoint. To address long-horizon credit assignment, we decompose rearrangement into stage-level subproblems using temporally decayed, stage-local completion rewards. Extensive simulation experiments demonstrate that EgoPush significantly outperforms end-to-end RL baselines in success rate, with ablation studies validating each design choice. We further demonstrate zero-shot sim-to-real transfer on a mobile platform in the real world. Code and videos are available at https://ai4ce.github.io/EgoPush/.",
      "upvotes": 2,
      "discussionId": "699bbee4f723198bd53a6307",
      "projectPage": "https://ai4ce.github.io/EgoPush/",
      "ai_summary": "EgoPush enables robot manipulation in cluttered environments through perception-driven policy learning that uses object-centric latent spaces and stage-decomposed rewards for long-horizon tasks.",
      "ai_keywords": [
        "policy learning",
        "egocentric perception",
        "non-prehensile rearrangement",
        "object-centric latent space",
        "reinforcement-learning",
        "visual student policy",
        "active perception",
        "stage-local completion rewards",
        "zero-shot sim-to-real transfer"
      ]
    },
    "publishedAt": "2026-02-20T03:54:20.000Z",
    "title": "EgoPush: Learning End-to-End Egocentric Multi-Object Rearrangement for Mobile Robots",
    "summary": "Humans can rearrange objects in cluttered environments using egocentric perception, navigating occlusions without global coordinates. Inspired by this capability, we study long-horizon multi-object non-prehensile rearrangement for mobile robots using a single egocentric camera. We introduce EgoPush, a policy learning framework that enables egocentric, perception-driven rearrangement without relying on explicit global state estimation that often fails in dynamic scenes. EgoPush designs an object-centric latent space to encode relative spatial relations among objects, rather than absolute poses. This design enables a privileged reinforcement-learning (RL) teacher to jointly learn latent states and mobile actions from sparse keypoints, which is then distilled into a purely visual student policy. To reduce the supervision gap between the omniscient teacher and the partially observed student, we restrict the teacher's observations to visually accessible cues. This induces active perception behaviors that are recoverable from the student's viewpoint. To address long-horizon credit assignment, we decompose rearrangement into stage-level subproblems using temporally decayed, stage-local completion rewards. Extensive simulation experiments demonstrate that EgoPush significantly outperforms end-to-end RL baselines in success rate, with ablation studies validating each design choice. We further demonstrate zero-shot sim-to-real transfer on a mobile platform in the real world. Code and videos are available at https://ai4ce.github.io/EgoPush/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/JZMXdQ95tWmAWkwvLCHIa.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18071.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 237,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.18432",
      "authors": [
        {
          "_id": "699bc2bff723198bd53a6330",
          "name": "Evonne Ng",
          "hidden": false
        },
        {
          "_id": "699bc2bff723198bd53a6331",
          "name": "Siwei Zhang",
          "hidden": false
        },
        {
          "_id": "699bc2bff723198bd53a6332",
          "name": "Zhang Chen",
          "hidden": false
        },
        {
          "_id": "699bc2bff723198bd53a6333",
          "name": "Michael Zollhoefer",
          "hidden": false
        },
        {
          "_id": "699bc2bff723198bd53a6334",
          "name": "Alexander Richard",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/SkRj7SyX6FxMHZgHAysQQ.mp4"
      ],
      "publishedAt": "2026-02-20T18:59:35.000Z",
      "submittedOnDailyAt": "2026-02-23T00:30:33.895Z",
      "title": "SARAH: Spatially Aware Real-time Agentic Humans",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on a streaming VR headset. Given a user's position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce a gaze scoring mechanism with classifier-free guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS -- 3x faster than non-causal baselines -- while capturing the subtle spatial dynamics of natural conversation. We validate our approach on a live VR system, bringing spatially-aware conversational agents to real-time deployment. Please see https://evonneng.github.io/sarah/ for details.",
      "upvotes": 1,
      "discussionId": "699bc2bff723198bd53a6335",
      "projectPage": "https://evonneng.github.io/sarah/",
      "ai_summary": "A causal transformer-based variational autoencoder combined with flow matching enables real-time, spatially-aware conversational motion for embodied agents in virtual reality applications.",
      "ai_keywords": [
        "causal transformer",
        "variational autoencoder",
        "flow matching model",
        "interleaved latent tokens",
        "classifier-free guidance",
        "spatial awareness",
        "conversational motion",
        "real-time inference",
        "embodied agents",
        "streaming VR"
      ]
    },
    "publishedAt": "2026-02-20T13:59:35.000Z",
    "title": "SARAH: Spatially Aware Real-time Agentic Humans",
    "summary": "As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on a streaming VR headset. Given a user's position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce a gaze scoring mechanism with classifier-free guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS -- 3x faster than non-causal baselines -- while capturing the subtle spatial dynamics of natural conversation. We validate our approach on a live VR system, bringing spatially-aware conversational agents to real-time deployment. Please see https://evonneng.github.io/sarah/ for details.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/SkRj7SyX6FxMHZgHAysQQ.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18432.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 237,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.18312",
      "authors": [
        {
          "_id": "699bbec7f723198bd53a62fb",
          "name": "Zhaoming Xie",
          "hidden": false
        },
        {
          "_id": "699bbec7f723198bd53a62fc",
          "name": "Kevin Karol",
          "hidden": false
        },
        {
          "_id": "699bbec7f723198bd53a62fd",
          "name": "Jessica Hodgins",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-20T16:11:19.000Z",
      "submittedOnDailyAt": "2026-02-23T00:13:26.091Z",
      "title": "Learning Smooth Time-Varying Linear Policies with an Action Jacobian Penalty",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Reinforcement learning provides a framework for learning control policies that can reproduce diverse motions for simulated characters. However, such policies often exploit unnatural high-frequency signals that are unachievable by humans or physical robots, making them poor representations of real-world behaviors. Existing work addresses this issue by adding a reward term that penalizes a large change in actions over time. This term often requires substantial tuning efforts. We propose to use the action Jacobian penalty, which penalizes changes in action with respect to the changes in simulated state directly through auto differentiation. This effectively eliminates unrealistic high-frequency control signals without task specific tuning. While effective, the action Jacobian penalty introduces significant computational overhead when used with traditional fully connected neural network architectures. To mitigate this, we introduce a new architecture called a Linear Policy Net (LPN) that significantly reduces the computational burden for calculating the action Jacobian penalty during training. In addition, a LPN requires no parameter tuning, exhibits faster learning convergence compared to baseline methods, and can be more efficiently queried during inference time compared to a fully connected neural network. We demonstrate that a Linear Policy Net, combined with the action Jacobian penalty, is able to learn policies that generate smooth signals while solving a number of motion imitation tasks with different characteristics, including dynamic motions such as a backflip and various challenging parkour skills. Finally, we apply this approach to create policies for dynamic motions on a physical quadrupedal robot equipped with an arm.",
      "upvotes": 0,
      "discussionId": "699bbec7f723198bd53a62fe",
      "ai_summary": "Reinforcement learning policies are improved by using action Jacobian penalty to eliminate unrealistic high-frequency signals, with a new Linear Policy Net architecture reducing computational overhead while enabling faster convergence and efficient inference for motion imitation tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "control policies",
        "action Jacobian penalty",
        "auto differentiation",
        "fully connected neural network",
        "Linear Policy Net",
        "motion imitation",
        "quadrupedal robot"
      ]
    },
    "publishedAt": "2026-02-20T11:11:19.000Z",
    "title": "Learning Smooth Time-Varying Linear Policies with an Action Jacobian Penalty",
    "summary": "Reinforcement learning provides a framework for learning control policies that can reproduce diverse motions for simulated characters. However, such policies often exploit unnatural high-frequency signals that are unachievable by humans or physical robots, making them poor representations of real-world behaviors. Existing work addresses this issue by adding a reward term that penalizes a large change in actions over time. This term often requires substantial tuning efforts. We propose to use the action Jacobian penalty, which penalizes changes in action with respect to the changes in simulated state directly through auto differentiation. This effectively eliminates unrealistic high-frequency control signals without task specific tuning. While effective, the action Jacobian penalty introduces significant computational overhead when used with traditional fully connected neural network architectures. To mitigate this, we introduce a new architecture called a Linear Policy Net (LPN) that significantly reduces the computational burden for calculating the action Jacobian penalty during training. In addition, a LPN requires no parameter tuning, exhibits faster learning convergence compared to baseline methods, and can be more efficiently queried during inference time compared to a fully connected neural network. We demonstrate that a Linear Policy Net, combined with the action Jacobian penalty, is able to learn policies that generate smooth signals while solving a number of motion imitation tasks with different characteristics, including dynamic motions such as a backflip and various challenging parkour skills. Finally, we apply this approach to create policies for dynamic motions on a physical quadrupedal robot equipped with an arm.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18312.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 237,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  }
]