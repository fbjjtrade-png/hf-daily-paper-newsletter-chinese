[
  {
    "paper": {
      "id": "2602.21193",
      "authors": [
        {
          "_id": "699e72b4dfbcf0b800aecb63",
          "name": "Renjie Pi",
          "hidden": false
        },
        {
          "_id": "699e72b4dfbcf0b800aecb64",
          "name": "Grace Lam",
          "hidden": false
        },
        {
          "_id": "699e72b4dfbcf0b800aecb65",
          "name": "Mohammad Shoeybi",
          "hidden": false
        },
        {
          "_id": "699e72b4dfbcf0b800aecb66",
          "name": "Pooya Jannaty",
          "hidden": false
        },
        {
          "_id": "699e72b4dfbcf0b800aecb67",
          "name": "Bryan Catanzaro",
          "hidden": false
        },
        {
          "_id": "699e72b4dfbcf0b800aecb68",
          "name": "Wei Ping",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-24T18:51:04.000Z",
      "submittedOnDailyAt": "2026-02-25T01:39:40.130Z",
      "title": "On Data Engineering for Scaling LLM Terminal Capabilities",
      "submittedOnDailyBy": {
        "_id": "63f45b8d520c14618930d175",
        "avatarUrl": "/avatars/42b3aaf50748a25e4a596fc57ab1306d.svg",
        "isPro": false,
        "fullname": "renjie",
        "user": "renjiepi",
        "type": "user"
      },
      "summary": "Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.",
      "upvotes": 24,
      "discussionId": "699e72b5dfbcf0b800aecb69",
      "projectPage": "https://huggingface.co/collections/nvidia/nemotron-terminal",
      "ai_summary": "Researchers developed a synthetic task generation pipeline and analyzed data strategies to improve terminal agent performance, creating a large-scale dataset and models that outperform larger counterparts on benchmark tests.",
      "ai_keywords": [
        "large language models",
        "terminal agents",
        "data engineering practices",
        "synthetic task generation",
        "Terminal-Task-Gen",
        "Terminal-Corpus",
        "Nemotron-Terminal",
        "Terminal-Bench 2.0",
        "curriculum learning",
        "long context training",
        "scaling behavior"
      ],
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2026-02-24T13:51:04.000Z",
    "title": "On Data Engineering for Scaling LLM Terminal Capabilities",
    "summary": "Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21193.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f45b8d520c14618930d175",
      "avatarUrl": "/avatars/42b3aaf50748a25e4a596fc57ab1306d.svg",
      "fullname": "renjie",
      "name": "renjiepi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.21015",
      "authors": [
        {
          "_id": "699e69fbdfbcf0b800aecafb",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "699e69fbdfbcf0b800aecafc",
          "name": "Maojia Song",
          "hidden": false
        },
        {
          "_id": "699e69fbdfbcf0b800aecafd",
          "name": "Yihuai Lan",
          "hidden": false
        },
        {
          "_id": "699e69fbdfbcf0b800aecafe",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "699e69fbdfbcf0b800aecaff",
          "name": "Zhiqiang Hu",
          "hidden": false
        },
        {
          "_id": "699e69fbdfbcf0b800aecb00",
          "name": "Yao Xiao",
          "hidden": false
        },
        {
          "_id": "699e69fbdfbcf0b800aecb01",
          "name": "Heng Zhou",
          "hidden": false
        },
        {
          "_id": "699e69fbdfbcf0b800aecb02",
          "name": "Weihua Zheng",
          "hidden": false
        },
        {
          "_id": "699e69fbdfbcf0b800aecb03",
          "name": "Dylan Raharja",
          "hidden": false
        },
        {
          "_id": "699e69fbdfbcf0b800aecb04",
          "name": "Soujanya Poria",
          "hidden": false
        },
        {
          "_id": "699e69fbdfbcf0b800aecb05",
          "name": "Roy Ka-Wei Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-24T15:33:02.000Z",
      "submittedOnDailyAt": "2026-02-25T00:55:31.629Z",
      "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning",
      "submittedOnDailyBy": {
        "_id": "637f228152229c63921119c3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg",
        "isPro": false,
        "fullname": "Zhiqiang Hu",
        "user": "Zhiqiang007",
        "type": "user"
      },
      "summary": "Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.",
      "upvotes": 18,
      "discussionId": "699e69fbdfbcf0b800aecb06",
      "projectPage": "https://social-ai-studio.github.io/CHAIN/",
      "githubRepo": "https://github.com/Social-AI-Studio/CHAIN",
      "githubRepoAddedBy": "user",
      "ai_summary": "Current vision-language models lack capability to understand physical structures and causal constraints needed for complex, interactive 3D tasks, as demonstrated by the CHAIN benchmark evaluating structured action planning under physical constraints.",
      "ai_keywords": [
        "Vision-Language Model",
        "diffusion-based models",
        "physical constraints",
        "causal constraints",
        "interactive 3D",
        "structured action sequences",
        "long-horizon planning"
      ],
      "githubStars": 1
    },
    "publishedAt": "2026-02-24T10:33:02.000Z",
    "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning",
    "summary": "Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21015.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "637f228152229c63921119c3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg",
      "fullname": "Zhiqiang Hu",
      "name": "Zhiqiang007",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.20739",
      "authors": [
        {
          "_id": "699e674ddfbcf0b800aecae9",
          "name": "Shitian Zhao",
          "hidden": false
        },
        {
          "_id": "699e674ddfbcf0b800aecaea",
          "name": "Shaoheng Lin",
          "hidden": false
        },
        {
          "_id": "699e674ddfbcf0b800aecaeb",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "699e674ddfbcf0b800aecaec",
          "name": "Haoquan Zhang",
          "hidden": false
        },
        {
          "_id": "699e674ddfbcf0b800aecaed",
          "name": "Wenshuo Peng",
          "hidden": false
        },
        {
          "_id": "699e674ddfbcf0b800aecaee",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "699e674ddfbcf0b800aecaef",
          "name": "Chen Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-24T10:08:33.000Z",
      "submittedOnDailyAt": "2026-02-25T00:41:13.309Z",
      "title": "PyVision-RL: Forging Open Agentic Vision Models via RL",
      "submittedOnDailyBy": {
        "_id": "62c66504031996c36c86976a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png",
        "isPro": false,
        "fullname": "steve z",
        "user": "stzhao",
        "type": "user"
      },
      "summary": "Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.",
      "upvotes": 17,
      "discussionId": "699e674edfbcf0b800aecaf0",
      "projectPage": "https://agent-x.space/pyvision-rl/",
      "githubRepo": "https://github.com/agents-x-project/PyVision-RL",
      "githubRepoAddedBy": "user",
      "ai_summary": "PyVision-RL framework addresses interaction collapse in multimodal models through enhanced reinforcement learning techniques and efficient video processing strategies.",
      "ai_keywords": [
        "reinforcement learning",
        "agentic multimodal models",
        "interaction collapse",
        "oversampling-filtering-ranking",
        "accumulative tool reward",
        "unified training pipeline",
        "on-demand context construction",
        "task-relevant frames",
        "visual token usage"
      ],
      "githubStars": 0
    },
    "publishedAt": "2026-02-24T05:08:33.000Z",
    "title": "PyVision-RL: Forging Open Agentic Vision Models via RL",
    "summary": "Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20739.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c66504031996c36c86976a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png",
      "fullname": "steve z",
      "name": "stzhao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.14337",
      "authors": [
        {
          "_id": "699cea804e37ec6dfa1bc476",
          "name": "Yukang Feng",
          "hidden": false
        },
        {
          "_id": "699cea804e37ec6dfa1bc477",
          "name": "Jianwen Sun",
          "hidden": false
        },
        {
          "_id": "699cea804e37ec6dfa1bc478",
          "name": "Zelai Yang",
          "hidden": false
        },
        {
          "_id": "699cea804e37ec6dfa1bc479",
          "name": "Jiaxin Ai",
          "hidden": false
        },
        {
          "_id": "699cea804e37ec6dfa1bc47a",
          "name": "Chuanhao Li",
          "hidden": false
        },
        {
          "_id": "699cea804e37ec6dfa1bc47b",
          "name": "Zizhen Li",
          "hidden": false
        },
        {
          "_id": "699cea804e37ec6dfa1bc47c",
          "name": "Fanrui Zhang",
          "hidden": false
        },
        {
          "_id": "699cea804e37ec6dfa1bc47d",
          "name": "Kang He",
          "hidden": false
        },
        {
          "_id": "699cea804e37ec6dfa1bc47e",
          "name": "Rui Ma",
          "hidden": false
        },
        {
          "_id": "699cea804e37ec6dfa1bc47f",
          "name": "Jifan Lin",
          "hidden": false
        },
        {
          "_id": "699cea804e37ec6dfa1bc480",
          "name": "Jie Sun",
          "hidden": false
        },
        {
          "_id": "699cea804e37ec6dfa1bc481",
          "name": "Yang Xiao",
          "hidden": false
        },
        {
          "_id": "699cea804e37ec6dfa1bc482",
          "name": "Sizhuo Zhou",
          "hidden": false
        },
        {
          "_id": "699cea804e37ec6dfa1bc483",
          "name": "Wenxiao Wu",
          "hidden": false
        },
        {
          "_id": "699cea804e37ec6dfa1bc484",
          "name": "Yiming Liu",
          "hidden": false
        },
        {
          "_id": "699cea804e37ec6dfa1bc485",
          "name": "Pengfei Liu",
          "hidden": false
        },
        {
          "_id": "699cea804e37ec6dfa1bc486",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "699cea804e37ec6dfa1bc487",
          "name": "Shenglin Zhang",
          "hidden": false
        },
        {
          "_id": "699cea804e37ec6dfa1bc488",
          "name": "Kaipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-15T23:12:57.000Z",
      "submittedOnDailyAt": "2026-02-25T01:19:35.669Z",
      "title": "LongCLI-Bench: A Preliminary Benchmark and Study for Long-horizon Agentic Programming in Command-Line Interfaces",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "Recent advances in AI-assisted programming have empowered agents to execute complex workflows via command-line interfaces, however, existing benchmarks are limited by short task horizons, data contamination from GitHub scraping, and a lack of fine-grained evaluation metrics, fail to rigorously evaluate the long-horizon planning and execution capabilities essential for realistic software engineering. To address these gaps, we introduce LongCLI-Bench, a comprehensive benchmark designed to evaluate agentic capabilities across long-horizon, realistic tasks. We curated 20 high-quality, long-horizon tasks from over 1,000 computer science assignments and real-world workflows, covering four engineering categories: from scratch, feature addition, bug fixing, and refactoring. We propose a dual-set testing protocol for LongCLI-Bench, which measures requirement fulfillment (fail-to-pass) and regression avoidance (pass-to-pass), and incorporates step-level scoring to pinpoint execution failures. Extensive experiments reveal that even state-of-the-art agents achieve pass rates below 20% in LongCLI-Bench. Step-level analysis further indicates that the majority of tasks stall at less than 30% completion, highlighting that critical failures often occur in the early stages. Although self-correction offers marginal gains, human-agent collaboration through plan injection and interactive guidance yields significantly higher improvements. These results highlight that future research must emphasize the development of synergistic human-agent workflows alongside advances in agents' planning and execution capabilities to overcome key challenges in long-horizon task performance.",
      "upvotes": 10,
      "discussionId": "699cea804e37ec6dfa1bc489",
      "projectPage": "https://github.com/finyorko/longcli-bench",
      "githubRepo": "https://github.com/finyorko/longcli-bench",
      "githubRepoAddedBy": "user",
      "ai_summary": "LongCLI-Bench evaluates AI agents' ability to complete complex, multi-step programming tasks through command-line interfaces with detailed failure analysis and human-agent collaboration insights.",
      "ai_keywords": [
        "AI-assisted programming",
        "command-line interfaces",
        "long-horizon planning",
        "agent evaluation",
        "requirement fulfillment",
        "regression avoidance",
        "step-level scoring",
        "self-correction",
        "human-agent collaboration",
        "plan injection",
        "interactive guidance"
      ],
      "githubStars": 2
    },
    "publishedAt": "2026-02-15T18:12:57.000Z",
    "title": "LongCLI-Bench: A Preliminary Benchmark and Study for Long-horizon Agentic Programming in Command-Line Interfaces",
    "summary": "Recent advances in AI-assisted programming have empowered agents to execute complex workflows via command-line interfaces, however, existing benchmarks are limited by short task horizons, data contamination from GitHub scraping, and a lack of fine-grained evaluation metrics, fail to rigorously evaluate the long-horizon planning and execution capabilities essential for realistic software engineering. To address these gaps, we introduce LongCLI-Bench, a comprehensive benchmark designed to evaluate agentic capabilities across long-horizon, realistic tasks. We curated 20 high-quality, long-horizon tasks from over 1,000 computer science assignments and real-world workflows, covering four engineering categories: from scratch, feature addition, bug fixing, and refactoring. We propose a dual-set testing protocol for LongCLI-Bench, which measures requirement fulfillment (fail-to-pass) and regression avoidance (pass-to-pass), and incorporates step-level scoring to pinpoint execution failures. Extensive experiments reveal that even state-of-the-art agents achieve pass rates below 20% in LongCLI-Bench. Step-level analysis further indicates that the majority of tasks stall at less than 30% completion, highlighting that critical failures often occur in the early stages. Although self-correction offers marginal gains, human-agent collaboration through plan injection and interactive guidance yields significantly higher improvements. These results highlight that future research must emphasize the development of synergistic human-agent workflows alongside advances in agents' planning and execution capabilities to overcome key challenges in long-horizon task performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14337.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.18940",
      "authors": [
        {
          "_id": "699e9a47dfbcf0b800aecbd8",
          "name": "Elad Ben Avraham",
          "hidden": false
        },
        {
          "_id": "699e9a47dfbcf0b800aecbd9",
          "name": "Changhao Li",
          "hidden": false
        },
        {
          "_id": "699e9a47dfbcf0b800aecbda",
          "name": "Ron Dorfman",
          "hidden": false
        },
        {
          "_id": "699e9a47dfbcf0b800aecbdb",
          "name": "Roy Ganz",
          "hidden": false
        },
        {
          "_id": "699e9a47dfbcf0b800aecbdc",
          "name": "Oren Nuriel",
          "hidden": false
        },
        {
          "_id": "699e9a47dfbcf0b800aecbdd",
          "name": "Amir Dudai",
          "hidden": false
        },
        {
          "_id": "699e9a47dfbcf0b800aecbde",
          "name": "Aviad Aberdam",
          "hidden": false
        },
        {
          "_id": "699e9a47dfbcf0b800aecbdf",
          "name": "Noah Flynn",
          "hidden": false
        },
        {
          "_id": "699e9a47dfbcf0b800aecbe0",
          "name": "Elman Mansimov",
          "hidden": false
        },
        {
          "_id": "699e9a47dfbcf0b800aecbe1",
          "name": "Adi Kalyanpur",
          "hidden": false
        },
        {
          "_id": "699e9a47dfbcf0b800aecbe2",
          "name": "Ron Litman",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-21T19:14:31.000Z",
      "submittedOnDailyAt": "2026-02-25T04:20:50.993Z",
      "title": "DREAM: Deep Research Evaluation with Agentic Metrics",
      "submittedOnDailyBy": {
        "_id": "62f0cb39671fc964b5063aba",
        "avatarUrl": "/avatars/02404cd78c395331b42500dd6ced35eb.svg",
        "isPro": false,
        "fullname": "Roy Ganz",
        "user": "proy",
        "type": "user"
      },
      "summary": "Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm.",
      "upvotes": 8,
      "discussionId": "699e9a47dfbcf0b800aecbe3",
      "ai_summary": "Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm.",
      "ai_keywords": [
        "Deep Research Agents",
        "analyst-grade reports",
        "Mirage of Synthesis",
        "tool-use capabilities",
        "temporal validity",
        "factual correctness",
        "DREAM",
        "agentic metrics",
        "evaluation protocol",
        "query-agnostic metrics",
        "adaptive metrics",
        "tool-calling agent",
        "temporally aware coverage",
        "grounded verification",
        "systematic reasoning probes"
      ],
      "organization": {
        "_id": "6058ec29102f61b42f65ae35",
        "name": "AWS",
        "fullname": "Amazon Web Services",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/wtuzZxWzijQ3do3zYoOFH.png"
      }
    },
    "publishedAt": "2026-02-21T14:14:31.000Z",
    "title": "DREAM: Deep Research Evaluation with Agentic Metrics",
    "summary": "Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18940.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f0cb39671fc964b5063aba",
      "avatarUrl": "/avatars/02404cd78c395331b42500dd6ced35eb.svg",
      "fullname": "Roy Ganz",
      "name": "proy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6058ec29102f61b42f65ae35",
      "name": "AWS",
      "fullname": "Amazon Web Services",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/wtuzZxWzijQ3do3zYoOFH.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.16990",
      "authors": [
        {
          "_id": "699e71d6dfbcf0b800aecb53",
          "name": "Yan Wang",
          "hidden": false
        },
        {
          "_id": "699e71d6dfbcf0b800aecb54",
          "name": "Yi Han",
          "hidden": false
        },
        {
          "_id": "699e71d6dfbcf0b800aecb55",
          "name": "Lingfei Qian",
          "hidden": false
        },
        {
          "_id": "699e71d6dfbcf0b800aecb56",
          "name": "Yueru He",
          "hidden": false
        },
        {
          "_id": "699e71d6dfbcf0b800aecb57",
          "name": "Xueqing Peng",
          "hidden": false
        },
        {
          "_id": "699e71d6dfbcf0b800aecb58",
          "name": "Dongji Feng",
          "hidden": false
        },
        {
          "_id": "699e71d6dfbcf0b800aecb59",
          "name": "Zhuohan Xie",
          "hidden": false
        },
        {
          "_id": "699e71d6dfbcf0b800aecb5a",
          "name": "Vincent Jim Zhang",
          "hidden": false
        },
        {
          "_id": "699e71d6dfbcf0b800aecb5b",
          "name": "Rosie Guo",
          "hidden": false
        },
        {
          "_id": "699e71d6dfbcf0b800aecb5c",
          "name": "Fengran Mo",
          "hidden": false
        },
        {
          "_id": "699e71d6dfbcf0b800aecb5d",
          "name": "Jimin Huang",
          "hidden": false
        },
        {
          "_id": "699e71d6dfbcf0b800aecb5e",
          "name": "Yankai Chen",
          "hidden": false
        },
        {
          "_id": "699e71d6dfbcf0b800aecb5f",
          "name": "Xue Liu",
          "hidden": false
        },
        {
          "_id": "699e71d6dfbcf0b800aecb60",
          "name": "Jian-Yun Nie",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-19T01:29:50.000Z",
      "submittedOnDailyAt": "2026-02-25T01:23:32.378Z",
      "title": "Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation",
      "submittedOnDailyBy": {
        "_id": "65d76cc5b9b7b8bf88faa916",
        "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
        "isPro": true,
        "fullname": "Yan Wang",
        "user": "YanAdjeNole",
        "type": "user"
      },
      "summary": "Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.",
      "upvotes": 7,
      "discussionId": "699e71d6dfbcf0b800aecb61",
      "githubRepo": "https://github.com/The-FinAI/Conv-FinRe",
      "githubRepoAddedBy": "user",
      "ai_summary": "A new conversational financial recommendation benchmark evaluates large language models' ability to balance rational decision-making with user behavior alignment using multi-view references derived from real market data and human decision trajectories.",
      "ai_keywords": [
        "recommendation benchmarks",
        "large language models",
        "financial advisory",
        "behavioral imitation",
        "decision quality",
        "multi-view references",
        "investor-specific risk preferences",
        "rational analysis",
        "market volatility",
        "user behavior alignment"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "658f4413674349122c0708e9",
        "name": "TheFinAI",
        "fullname": "The Fin AI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
      }
    },
    "publishedAt": "2026-02-18T20:29:50.000Z",
    "title": "Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation",
    "summary": "Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d76cc5b9b7b8bf88faa916",
      "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
      "fullname": "Yan Wang",
      "name": "YanAdjeNole",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "658f4413674349122c0708e9",
      "name": "TheFinAI",
      "fullname": "The Fin AI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/ZK5nQKw34W3-eH3p4NAYc.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.21198",
      "authors": [
        {
          "_id": "699e7520dfbcf0b800aecb7c",
          "name": "Yining Hong",
          "hidden": false
        },
        {
          "_id": "699e7520dfbcf0b800aecb7d",
          "name": "Huang Huang",
          "hidden": false
        },
        {
          "_id": "699e7520dfbcf0b800aecb7e",
          "name": "Manling Li",
          "hidden": false
        },
        {
          "_id": "699e7520dfbcf0b800aecb7f",
          "name": "Li Fei-Fei",
          "hidden": false
        },
        {
          "_id": "699e7520dfbcf0b800aecb80",
          "name": "Jiajun Wu",
          "hidden": false
        },
        {
          "_id": "699e7520dfbcf0b800aecb81",
          "name": "Yejin Choi",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6431b64df76c34519e93d1ba/oSZG-BmjUOWwm3C3QdUGm.png"
      ],
      "publishedAt": "2026-02-24T18:55:18.000Z",
      "submittedOnDailyAt": "2026-02-25T01:37:43.503Z",
      "title": "Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs",
      "submittedOnDailyBy": {
        "_id": "6431b64df76c34519e93d1ba",
        "avatarUrl": "/avatars/ea577762b6b4798f87a7a3f1d53d082c.svg",
        "isPro": false,
        "fullname": "Yining Hong",
        "user": "evelynhong",
        "type": "user"
      },
      "summary": "Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: reflection-in-action, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and reflection-on-action, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.",
      "upvotes": 3,
      "discussionId": "699e7521dfbcf0b800aecb82",
      "projectPage": "https://reflective-test-time-planning.github.io/",
      "ai_summary": "Reflective Test-Time Planning enhances robot decision-making by integrating multiple reflection mechanisms that enable learning from experience and improving long-horizon task performance.",
      "ai_keywords": [
        "embodied LLMs",
        "test-time planning",
        "reflection-in-action",
        "reflection-on-action",
        "retrospective reflection",
        "test-time scaling",
        "test-time training",
        "long-horizon task reasoning",
        "behavioral correction"
      ]
    },
    "publishedAt": "2026-02-24T13:55:18.000Z",
    "title": "Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs",
    "summary": "Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: reflection-in-action, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and reflection-on-action, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6431b64df76c34519e93d1ba/oSZG-BmjUOWwm3C3QdUGm.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21198.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6431b64df76c34519e93d1ba",
      "avatarUrl": "/avatars/ea577762b6b4798f87a7a3f1d53d082c.svg",
      "fullname": "Yining Hong",
      "name": "evelynhong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.21185",
      "authors": [
        {
          "_id": "699e7bb0dfbcf0b800aecb84",
          "name": "Justin Deschenaux",
          "hidden": false
        },
        {
          "_id": "699e7bb0dfbcf0b800aecb85",
          "name": "Caglar Gulcehre",
          "hidden": false
        },
        {
          "_id": "699e7bb0dfbcf0b800aecb86",
          "name": "Subham Sekhar Sahoo",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-24T18:35:22.000Z",
      "submittedOnDailyAt": "2026-02-25T02:03:58.889Z",
      "title": "The Diffusion Duality, Chapter II: Ψ-Samplers and Efficient Curriculum",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these settings. However, their sampling quality plateaus with ancestral samplers as the number of steps increases. We introduce a family of Predictor-Corrector (PC) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. When paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on OpenWebText and better FID/IS scores on CIFAR10. Crucially, unlike conventional samplers, our PC methods continue to improve with more sampling steps. Taken together, these findings call into question the assumption that Masked diffusion is the inevitable future of diffusion-based language modeling. Beyond sampling, we develop a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to Duo while maintaining comparable perplexity on OpenWebText and LM1B and strong downstream performance. We release code, checkpoints, and a video-tutorial on: https://s-sahoo.com/duo-ch2",
      "upvotes": 2,
      "discussionId": "699e7bb0dfbcf0b800aecb87",
      "projectPage": "https://s-sahoo.com/duo-ch2/",
      "ai_summary": "Discrete diffusion models with predictor-corrector samplers surpass traditional methods in generation quality and efficiency, challenging assumptions about masked diffusion's necessity in language modeling.",
      "ai_keywords": [
        "discrete diffusion models",
        "predictor-corrector samplers",
        "ancestral sampling",
        "generative perplexity",
        "unigram entropy",
        "FID",
        "IS scores",
        "Gaussian relaxation",
        "curriculum training",
        "Masked diffusion"
      ]
    },
    "publishedAt": "2026-02-24T13:35:22.000Z",
    "title": "The Diffusion Duality, Chapter II: Ψ-Samplers and Efficient Curriculum",
    "summary": "Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these settings. However, their sampling quality plateaus with ancestral samplers as the number of steps increases. We introduce a family of Predictor-Corrector (PC) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. When paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on OpenWebText and better FID/IS scores on CIFAR10. Crucially, unlike conventional samplers, our PC methods continue to improve with more sampling steps. Taken together, these findings call into question the assumption that Masked diffusion is the inevitable future of diffusion-based language modeling. Beyond sampling, we develop a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to Duo while maintaining comparable perplexity on OpenWebText and LM1B and strong downstream performance. We release code, checkpoints, and a video-tutorial on: https://s-sahoo.com/duo-ch2",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21185.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 239,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.21201",
      "authors": [
        {
          "_id": "699e7101dfbcf0b800aecb40",
          "name": "Tony Feng",
          "hidden": false
        },
        {
          "_id": "699e7101dfbcf0b800aecb41",
          "name": "Junehyuk Jung",
          "hidden": false
        },
        {
          "_id": "699e7101dfbcf0b800aecb42",
          "name": "Sang-hyun Kim",
          "hidden": false
        },
        {
          "_id": "699e7101dfbcf0b800aecb43",
          "name": "Carlo Pagano",
          "hidden": false
        },
        {
          "_id": "699e7101dfbcf0b800aecb44",
          "name": "Sergei Gukov",
          "hidden": false
        },
        {
          "_id": "699e7101dfbcf0b800aecb45",
          "name": "Chiang-Chiang Tsai",
          "hidden": false
        },
        {
          "_id": "699e7101dfbcf0b800aecb46",
          "name": "David Woodruff",
          "hidden": false
        },
        {
          "_id": "699e7101dfbcf0b800aecb47",
          "name": "Adel Javanmard",
          "hidden": false
        },
        {
          "_id": "699e7101dfbcf0b800aecb48",
          "name": "Aryan Mokhtari",
          "hidden": false
        },
        {
          "_id": "699e7101dfbcf0b800aecb49",
          "name": "Dawsen Hwang",
          "hidden": false
        },
        {
          "_id": "699e7101dfbcf0b800aecb4a",
          "name": "Yuri Chervonyi",
          "hidden": false
        },
        {
          "_id": "699e7101dfbcf0b800aecb4b",
          "name": "Jonathan N. Lee",
          "hidden": false
        },
        {
          "_id": "699e7101dfbcf0b800aecb4c",
          "name": "Garrett Bingham",
          "hidden": false
        },
        {
          "_id": "699e7101dfbcf0b800aecb4d",
          "name": "Trieu H. Trinh",
          "hidden": false
        },
        {
          "_id": "699e7101dfbcf0b800aecb4e",
          "name": "Vahab Mirrokni",
          "hidden": false
        },
        {
          "_id": "699e7101dfbcf0b800aecb4f",
          "name": "Quoc V. Le",
          "hidden": false
        },
        {
          "_id": "699e7101dfbcf0b800aecb50",
          "name": "Thang Luong",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-24T18:56:10.000Z",
      "submittedOnDailyAt": "2026-02-25T01:42:48.338Z",
      "title": "Aletheia tackles FirstProof autonomously",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on Problem 8 (only). For full transparency, we explain our interpretation of FirstProof and disclose details about our experiments as well as our evaluation. Raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia.",
      "upvotes": 1,
      "discussionId": "699e7101dfbcf0b800aecb51",
      "projectPage": "https://github.com/google-deepmind/superhuman/tree/main/aletheia",
      "organization": {
        "_id": "5e6aca39878b8b2bf9806447",
        "name": "google",
        "fullname": "Google",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
      }
    },
    "publishedAt": "2026-02-24T13:56:10.000Z",
    "title": "Aletheia tackles FirstProof autonomously",
    "summary": "We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on Problem 8 (only). For full transparency, we explain our interpretation of FirstProof and disclose details about our experiments as well as our evaluation. Raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21201.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 239,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5e6aca39878b8b2bf9806447",
      "name": "google",
      "fullname": "Google",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.21196",
      "authors": [
        {
          "_id": "699e9f0bdfbcf0b800aecbf4",
          "name": "Ravi Ghadia",
          "hidden": false
        },
        {
          "_id": "699e9f0bdfbcf0b800aecbf5",
          "name": "Maksim Abraham",
          "hidden": false
        },
        {
          "_id": "699e9f0bdfbcf0b800aecbf6",
          "name": "Sergei Vorobyov",
          "hidden": false
        },
        {
          "_id": "699e9f0bdfbcf0b800aecbf7",
          "name": "Max Ryabinin",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-24T18:54:39.000Z",
      "submittedOnDailyAt": "2026-02-25T04:41:04.657Z",
      "title": "Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking",
      "submittedOnDailyBy": {
        "_id": "607d59fb921db717010c7ccc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625736058289-607d59fb921db717010c7ccc.png",
        "isPro": false,
        "fullname": "Max Ryabinin",
        "user": "mryab",
        "type": "user"
      },
      "summary": "Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend the possible context length at the cost of training throughput. In this paper, we present UPipe, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level. This technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths. Our approach reduces intermediate tensor memory usage in the attention layer by as much as 87.5% for 32B Transformers, while matching previous context parallelism techniques in terms of training speed. UPipe can support the context length of 5M tokens when training Llama3-8B on a single 8timesH100 node, improving upon prior methods by over 25%.",
      "upvotes": 1,
      "discussionId": "699e9f0bdfbcf0b800aecbf8",
      "projectPage": "https://rghadia.github.io/untied_ulysses_proj/",
      "githubRepo": "https://github.com/togethercomputer/Untied-Ulysses",
      "githubRepoAddedBy": "user",
      "ai_summary": "UPipe enables efficient processing of long sequences in Transformer models through fine-grained chunking at the attention head level, significantly reducing activation memory usage while maintaining training speed.",
      "ai_keywords": [
        "Transformer models",
        "context parallelism",
        "Ring Attention",
        "DeepSpeed Ulysses",
        "Fully Pipelined Distributed Transformer",
        "activation offloading",
        "self-attention",
        "intermediate tensor memory usage",
        "attention head level chunking"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "632b803bb2dd35f135623cc2",
        "name": "togethercomputer",
        "fullname": "Together",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678734258201-6322ad266b1992383fa964ca.png"
      }
    },
    "publishedAt": "2026-02-24T13:54:39.000Z",
    "title": "Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking",
    "summary": "Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend the possible context length at the cost of training throughput. In this paper, we present UPipe, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level. This technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths. Our approach reduces intermediate tensor memory usage in the attention layer by as much as 87.5% for 32B Transformers, while matching previous context parallelism techniques in terms of training speed. UPipe can support the context length of 5M tokens when training Llama3-8B on a single 8timesH100 node, improving upon prior methods by over 25%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21196.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "607d59fb921db717010c7ccc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625736058289-607d59fb921db717010c7ccc.png",
      "fullname": "Max Ryabinin",
      "name": "mryab",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 30,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "632b803bb2dd35f135623cc2",
      "name": "togethercomputer",
      "fullname": "Together",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678734258201-6322ad266b1992383fa964ca.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.20945",
      "authors": [
        {
          "_id": "699e6691dfbcf0b800aecadc",
          "name": "Taiqiang Wu",
          "hidden": false
        },
        {
          "_id": "699e6691dfbcf0b800aecadd",
          "name": "Zenan Zu",
          "hidden": false
        },
        {
          "_id": "699e6691dfbcf0b800aecade",
          "name": "Bo Zhou",
          "hidden": false
        },
        {
          "_id": "699e6691dfbcf0b800aecadf",
          "name": "Ngai Wong",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-24T14:28:16.000Z",
      "submittedOnDailyAt": "2026-02-25T00:33:51.481Z",
      "title": "The Art of Efficient Reasoning: Data, Reward, and Optimization",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.",
      "upvotes": 1,
      "discussionId": "699e6691dfbcf0b800aecae0",
      "ai_summary": "Large language models benefit from scaled chain-of-thought reasoning through efficient training methods that balance trajectory length and accuracy using reinforcement learning with reward shaping.",
      "ai_keywords": [
        "Chain-of-Thought",
        "reinforcement learning",
        "reward shaping",
        "length adaptation",
        "reasoning refinement",
        "token budget",
        "length collapse",
        "generalization"
      ],
      "organization": {
        "_id": "6645f953c39288df638dbdd5",
        "name": "Tencent-Hunyuan",
        "fullname": "Tencent Hunyuan",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
      }
    },
    "publishedAt": "2026-02-24T09:28:16.000Z",
    "title": "The Art of Efficient Reasoning: Data, Reward, and Optimization",
    "summary": "Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20945.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 239,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6645f953c39288df638dbdd5",
      "name": "Tencent-Hunyuan",
      "fullname": "Tencent Hunyuan",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.20424",
      "authors": [
        {
          "_id": "699e70a9dfbcf0b800aecb2d",
          "name": "Ved Sirdeshmukh",
          "hidden": false
        },
        {
          "_id": "699e70a9dfbcf0b800aecb2e",
          "name": "Marc Wetter",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-23T23:46:55.000Z",
      "submittedOnDailyAt": "2026-02-25T01:17:17.713Z",
      "title": "Implicit Intelligence -- Evaluating Agents on What Users Don't Say",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Real-world requests to AI agents are fundamentally underspecified. Natural human communication relies on shared context and unstated constraints that speakers expect listeners to infer. Current agentic benchmarks test explicit instruction-following but fail to evaluate whether agents can reason about implicit requirements spanning accessibility needs, privacy boundaries, catastrophic risks, and contextual constraints. We present Implicit Intelligence, an evaluation framework testing whether AI agents can move beyond prompt-following to become genuine goal-fulfillers, paired with Agent-as-a-World (AaW), a harness where interactive worlds are defined in human-readable YAML files and simulated by language models. Our scenarios feature apparent simplicity in user requests, hidden complexity in correct solutions, and discoverability of constraints through environmental exploration. Evaluating 16 frontier and open-weight models across 205 scenarios, we find that even the best-performing model achieves only 48.3% scenario pass rate, revealing substantial room for improvement in bridging the gap between literal instruction-following and human-like contextual reasoning.",
      "upvotes": 1,
      "discussionId": "699e70aadfbcf0b800aecb2f",
      "ai_summary": "AI agents struggle to interpret implicitly specified real-world requests that require contextual reasoning beyond explicit instructions, as demonstrated by an evaluation framework using interactive YAML-defined worlds.",
      "ai_keywords": [
        "AI agents",
        "implicit requirements",
        "contextual reasoning",
        "instruction-following",
        "evaluation framework",
        "Agent-as-a-World",
        "YAML files",
        "language models",
        "interactive worlds",
        "scenario pass rate"
      ]
    },
    "publishedAt": "2026-02-23T18:46:55.000Z",
    "title": "Implicit Intelligence -- Evaluating Agents on What Users Don't Say",
    "summary": "Real-world requests to AI agents are fundamentally underspecified. Natural human communication relies on shared context and unstated constraints that speakers expect listeners to infer. Current agentic benchmarks test explicit instruction-following but fail to evaluate whether agents can reason about implicit requirements spanning accessibility needs, privacy boundaries, catastrophic risks, and contextual constraints. We present Implicit Intelligence, an evaluation framework testing whether AI agents can move beyond prompt-following to become genuine goal-fulfillers, paired with Agent-as-a-World (AaW), a harness where interactive worlds are defined in human-readable YAML files and simulated by language models. Our scenarios feature apparent simplicity in user requests, hidden complexity in correct solutions, and discoverability of constraints through environmental exploration. Evaluating 16 frontier and open-weight models across 205 scenarios, we find that even the best-performing model achieves only 48.3% scenario pass rate, revealing substantial room for improvement in bridging the gap between literal instruction-following and human-like contextual reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20424.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 239,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.18735",
      "authors": [
        {
          "_id": "699d85ab9491ce7a9c6889ef",
          "name": "Weilong Yan",
          "hidden": false
        },
        {
          "_id": "699d85ab9491ce7a9c6889f0",
          "name": "Haipeng Li",
          "hidden": false
        },
        {
          "_id": "699d85ab9491ce7a9c6889f1",
          "name": "Hao Xu",
          "hidden": false
        },
        {
          "_id": "699d85ab9491ce7a9c6889f2",
          "name": "Nianjin Ye",
          "hidden": false
        },
        {
          "_id": "699d85ab9491ce7a9c6889f3",
          "name": "Yihao Ai",
          "hidden": false
        },
        {
          "_id": "699d85ab9491ce7a9c6889f4",
          "name": "Shuaicheng Liu",
          "hidden": false
        },
        {
          "_id": "699d85ab9491ce7a9c6889f5",
          "name": "Jingyu Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-21T06:55:28.000Z",
      "submittedOnDailyAt": "2026-02-25T04:52:26.065Z",
      "title": "LaS-Comp: Zero-shot 3D Completion with Latent-Spatial Consistency",
      "submittedOnDailyBy": {
        "_id": "668192a3f35c3ff47a8438ee",
        "avatarUrl": "/avatars/6b293341f5dc51f574252c6f57cfd293.svg",
        "isPro": false,
        "fullname": "Weilong Yan",
        "user": "DavidYan2001",
        "type": "user"
      },
      "summary": "This paper introduces LaS-Comp, a zero-shot and category-agnostic approach that leverages the rich geometric priors of 3D foundation models to enable 3D shape completion across diverse types of partial observations. Our contributions are threefold: First,  harnesses these powerful generative priors for completion through a complementary two-stage design: (i) an explicit replacement stage that preserves the partial observation geometry to ensure faithful completion; and (ii) an implicit refinement stage ensures seamless boundaries between the observed and synthesized regions. Second, our framework is training-free and compatible with different 3D foundation models. Third, we introduce Omni-Comp, a comprehensive benchmark combining real-world and synthetic data with diverse and challenging partial patterns, enabling a more thorough and realistic evaluation. Both quantitative and qualitative experiments demonstrate that our approach outperforms previous state-of-the-art approaches. Our code and data will be available at https://github.com/DavidYan2001/LaS-Comp{LaS-Comp}.",
      "upvotes": 1,
      "discussionId": "699d85ab9491ce7a9c6889f6",
      "githubRepo": "https://github.com/DavidYan2001/LaS-Comp",
      "githubRepoAddedBy": "user",
      "ai_summary": "LaS-Comp presents a zero-shot 3D shape completion method using 3D foundation models with a two-stage approach for faithful reconstruction and seamless boundary refinement.",
      "ai_keywords": [
        "3D shape completion",
        "3D foundation models",
        "geometric priors",
        "zero-shot",
        "category-agnostic",
        "two-stage design",
        "explicit replacement stage",
        "implicit refinement stage",
        "training-free",
        "Omni-Comp",
        "comprehensive benchmark"
      ],
      "githubStars": 9,
      "organization": {
        "_id": "6508ab2b349930913196378b",
        "name": "NationalUniversityofSingapore",
        "fullname": "National University of Singapore",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
      }
    },
    "publishedAt": "2026-02-21T01:55:28.000Z",
    "title": "LaS-Comp: Zero-shot 3D Completion with Latent-Spatial Consistency",
    "summary": "This paper introduces LaS-Comp, a zero-shot and category-agnostic approach that leverages the rich geometric priors of 3D foundation models to enable 3D shape completion across diverse types of partial observations. Our contributions are threefold: First,  harnesses these powerful generative priors for completion through a complementary two-stage design: (i) an explicit replacement stage that preserves the partial observation geometry to ensure faithful completion; and (ii) an implicit refinement stage ensures seamless boundaries between the observed and synthesized regions. Second, our framework is training-free and compatible with different 3D foundation models. Third, we introduce Omni-Comp, a comprehensive benchmark combining real-world and synthetic data with diverse and challenging partial patterns, enabling a more thorough and realistic evaluation. Both quantitative and qualitative experiments demonstrate that our approach outperforms previous state-of-the-art approaches. Our code and data will be available at https://github.com/DavidYan2001/LaS-Comp{LaS-Comp}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668192a3f35c3ff47a8438ee",
      "avatarUrl": "/avatars/6b293341f5dc51f574252c6f57cfd293.svg",
      "fullname": "Weilong Yan",
      "name": "DavidYan2001",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6508ab2b349930913196378b",
      "name": "NationalUniversityofSingapore",
      "fullname": "National University of Singapore",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.16813",
      "authors": [
        {
          "_id": "699da378b767f258d0bf1a82",
          "name": "Chanhyuk Lee",
          "hidden": false
        },
        {
          "_id": "699da378b767f258d0bf1a83",
          "name": "Jaehoon Yoo",
          "hidden": false
        },
        {
          "_id": "699da378b767f258d0bf1a84",
          "name": "Manan Agarwal",
          "hidden": false
        },
        {
          "_id": "699da378b767f258d0bf1a85",
          "name": "Sheel Shah",
          "hidden": false
        },
        {
          "_id": "699da378b767f258d0bf1a86",
          "name": "Jerry Huang",
          "hidden": false
        },
        {
          "_id": "699da378b767f258d0bf1a87",
          "name": "Aditi Raghunathan",
          "hidden": false
        },
        {
          "_id": "699da378b767f258d0bf1a88",
          "name": "Seunghoon Hong",
          "hidden": false
        },
        {
          "_id": "699da378b767f258d0bf1a89",
          "name": "Nicholas M. Boffi",
          "hidden": false
        },
        {
          "_id": "699da378b767f258d0bf1a8a",
          "name": "Jinwoo Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-18T19:23:07.000Z",
      "submittedOnDailyAt": "2026-02-25T04:51:51.814Z",
      "title": "One-step Language Modeling via Continuous Denoising",
      "submittedOnDailyBy": {
        "_id": "63e839ede02ee67e8e58eaf6",
        "avatarUrl": "/avatars/72512273f0e1a4469a2be16c9cfacaa1.svg",
        "isPro": false,
        "fullname": "Chanhyuk David Lee",
        "user": "david3684",
        "type": "user"
      },
      "summary": "Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings. We show that the model can be trained by predicting the clean data via a cross entropy objective, where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation. On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities, and paves the way toward accelerated flow-based language modeling at scale. Code is available at https://github.com/david3684/flm.",
      "upvotes": 1,
      "discussionId": "699da378b767f258d0bf1a8d",
      "projectPage": "https://one-step-lm.github.io/",
      "githubRepo": "https://github.com/david3684/flm",
      "githubRepoAddedBy": "user",
      "ai_summary": "Flow-based language models outperform discrete diffusion models in both quality and speed by using Euclidean denoising over one-hot token encodings with improved training stability through time reparameterization.",
      "ai_keywords": [
        "language models",
        "discrete diffusion",
        "flow-based continuous denoising",
        "Euclidean denoising",
        "one-hot token encodings",
        "cross entropy objective",
        "time reparameterization",
        "distilled flow map",
        "few-step generation",
        "discrete modalities",
        "generative modeling"
      ],
      "githubStars": 42
    },
    "publishedAt": "2026-02-18T14:23:07.000Z",
    "title": "One-step Language Modeling via Continuous Denoising",
    "summary": "Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings. We show that the model can be trained by predicting the clean data via a cross entropy objective, where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation. On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities, and paves the way toward accelerated flow-based language modeling at scale. Code is available at https://github.com/david3684/flm.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16813.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e839ede02ee67e8e58eaf6",
      "avatarUrl": "/avatars/72512273f0e1a4469a2be16c9cfacaa1.svg",
      "fullname": "Chanhyuk David Lee",
      "name": "david3684",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.16603",
      "authors": [
        {
          "_id": "699c66c0cf1450b05134df07",
          "user": {
            "_id": "662dfa31d9b837e4b93dcf11",
            "avatarUrl": "/avatars/470e738a720660f6127aa09f09e0a880.svg",
            "isPro": false,
            "fullname": "hsiehchiachi",
            "user": "hsiehchiachi",
            "type": "user"
          },
          "name": "Chia-chi Hsieh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-24T09:51:32.902Z",
          "hidden": false
        },
        {
          "_id": "699c66c0cf1450b05134df08",
          "name": "Zan Zong",
          "hidden": false
        },
        {
          "_id": "699c66c0cf1450b05134df09",
          "name": "Xinyang Chen",
          "hidden": false
        },
        {
          "_id": "699c66c0cf1450b05134df0a",
          "name": "Jianjiang Li",
          "hidden": false
        },
        {
          "_id": "699c66c0cf1450b05134df0b",
          "name": "Jidong Zhai",
          "hidden": false
        },
        {
          "_id": "699c66c0cf1450b05134df0c",
          "name": "Lijie Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-18T16:57:45.000Z",
      "submittedOnDailyAt": "2026-02-25T01:01:54.235Z",
      "title": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving",
      "submittedOnDailyBy": {
        "_id": "662dfa31d9b837e4b93dcf11",
        "avatarUrl": "/avatars/470e738a720660f6127aa09f09e0a880.svg",
        "isPro": false,
        "fullname": "hsiehchiachi",
        "user": "hsiehchiachi",
        "type": "user"
      },
      "summary": "The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge.\n  In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6times compared to state-of-the-art systems while satisfying heterogeneous SLOs.",
      "upvotes": 1,
      "discussionId": "699c66c0cf1450b05134df0d",
      "githubRepo": "https://github.com/HSIEHCHIACHI/FlowPrefill",
      "githubRepoAddedBy": "user",
      "ai_summary": "FlowPrefill addresses head-of-line blocking in large language model serving by decoupling preemption granularity from scheduling frequency through operator-level preemption and event-driven scheduling, achieving up to 5.6x better goodput than existing systems.",
      "ai_keywords": [
        "large language models",
        "prefill phase",
        "head-of-line blocking",
        "time-to-first-token",
        "preemption granularity",
        "scheduling frequency",
        "operator-level preemption",
        "event-driven scheduling",
        "goodput",
        "service level objectives"
      ],
      "githubStars": 3
    },
    "publishedAt": "2026-02-18T11:57:45.000Z",
    "title": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving",
    "summary": "The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge.\n  In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6times compared to state-of-the-art systems while satisfying heterogeneous SLOs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16603.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "662dfa31d9b837e4b93dcf11",
      "avatarUrl": "/avatars/470e738a720660f6127aa09f09e0a880.svg",
      "fullname": "hsiehchiachi",
      "name": "hsiehchiachi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.21204",
      "authors": [
        {
          "_id": "699e6e60dfbcf0b800aecb16",
          "name": "Junchen Liu",
          "hidden": false
        },
        {
          "_id": "699e6e60dfbcf0b800aecb17",
          "name": "Sven Elflein",
          "hidden": false
        },
        {
          "_id": "699e6e60dfbcf0b800aecb18",
          "name": "Or Litany",
          "hidden": false
        },
        {
          "_id": "699e6e60dfbcf0b800aecb19",
          "name": "Zan Gojcic",
          "hidden": false
        },
        {
          "_id": "699e6e60dfbcf0b800aecb1a",
          "name": "Ruilong Li",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-24T18:59:30.000Z",
      "submittedOnDailyAt": "2026-02-25T03:15:10.854Z",
      "title": "Test-Time Training with KV Binding Is Secretly Linear Attention",
      "submittedOnDailyBy": {
        "_id": "663a9ae58cf658ffaf3d02e5",
        "avatarUrl": "/avatars/aa35668dc088e675e794b9ceb935c56f.svg",
        "isPro": false,
        "fullname": "Junchen Liu",
        "user": "JunchenLiu",
        "type": "user"
      },
      "summary": "Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.",
      "upvotes": 0,
      "discussionId": "699e6e61dfbcf0b800aecb1b",
      "projectPage": "https://research.nvidia.com/labs/sil/projects/tttla/",
      "ai_summary": "Test-time training is reinterpreted as learned linear attention rather than memorization, offering architectural simplifications and improved efficiency.",
      "ai_keywords": [
        "test-time training",
        "KV binding",
        "online meta-learning",
        "learned linear attention",
        "sequence modeling layer",
        "linear attention operator",
        "architectural simplifications",
        "parallel formulations",
        "representational capacity"
      ],
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2026-02-24T13:59:30.000Z",
    "title": "Test-Time Training with KV Binding Is Secretly Linear Attention",
    "summary": "Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21204.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "663a9ae58cf658ffaf3d02e5",
      "avatarUrl": "/avatars/aa35668dc088e675e794b9ceb935c56f.svg",
      "fullname": "Junchen Liu",
      "name": "JunchenLiu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.21053",
      "authors": [
        {
          "_id": "699e989bdfbcf0b800aecbc6",
          "name": "Shimin Wen",
          "hidden": false
        },
        {
          "_id": "699e989bdfbcf0b800aecbc7",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "699e989bdfbcf0b800aecbc8",
          "name": "Xingdou Bian",
          "hidden": false
        },
        {
          "_id": "699e989bdfbcf0b800aecbc9",
          "name": "Hongjie Zhu",
          "hidden": false
        },
        {
          "_id": "699e989bdfbcf0b800aecbca",
          "name": "Lulu He",
          "hidden": false
        },
        {
          "_id": "699e989bdfbcf0b800aecbcb",
          "name": "Layi Shama",
          "hidden": false
        },
        {
          "_id": "699e989bdfbcf0b800aecbcc",
          "name": "Daji Ergu",
          "hidden": false
        },
        {
          "_id": "699e989bdfbcf0b800aecbcd",
          "name": "Ying Cai",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-24T16:10:27.000Z",
      "submittedOnDailyAt": "2026-02-25T04:07:41.939Z",
      "title": "OCR-Agent: Agentic OCR with Capability and Memory Reflection",
      "submittedOnDailyBy": {
        "_id": "64ec877bb93654d4ca5c92e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
        "isPro": false,
        "fullname": "Zeyu Zhang",
        "user": "SteveZeyuZhang",
        "type": "user"
      },
      "summary": "Large Vision-Language Models (VLMs) have demonstrated significant potential on complex visual understanding tasks through iterative optimization methods.However, these models generally lack effective self-correction mechanisms, making it difficult for them to independently rectify cognitive biases. Consequently, during multi-turn revisions, they often fall into repetitive and ineffective attempts, failing to achieve stable improvements in answer quality.To address this issue, we propose a novel iterative self-correction framework that endows models with two key capabilities: Capability Reflection and Memory Reflection. This framework guides the model to first diagnose errors and generate a correction plan via Capability Reflection, then leverage Memory Reflection to review past attempts to avoid repetition and explore new solutions, and finally, optimize the answer through rigorous re-reasoning. Experiments on the challenging OCRBench v2 benchmark show that OCR-Agent outperforms the current open-source SOTA model InternVL3-8B by +2.0 on English and +1.2 on Chinese subsets, while achieving state-of-the-art results in Visual Understanding (79.9) and Reasoning (66.5) - surpassing even larger fine-tuned models. Our method demonstrates that structured, self-aware reflection can significantly enhance VLMs' reasoning robustness without additional training. Code: https://github.com/AIGeeksGroup/OCR-Agent.",
      "upvotes": 0,
      "discussionId": "699e989bdfbcf0b800aecbce",
      "githubRepo": "https://github.com/AIGeeksGroup/OCR-Agent",
      "githubRepoAddedBy": "user",
      "ai_summary": "A novel iterative self-correction framework enhances vision-language models' reasoning robustness through capability reflection and memory reflection mechanisms, achieving superior performance on visual understanding benchmarks.",
      "ai_keywords": [
        "vision-language models",
        "iterative optimization",
        "self-correction mechanisms",
        "capability reflection",
        "memory reflection",
        "re-reasoning",
        "OCRBench v2",
        "Visual Understanding",
        "Reasoning"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "686804a93cd67b3369df8182",
        "name": "AIGeeksGroup",
        "fullname": "AI Geeks",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/cyfudFg0OeqgwcLHwjeCc.png"
      }
    },
    "publishedAt": "2026-02-24T11:10:27.000Z",
    "title": "OCR-Agent: Agentic OCR with Capability and Memory Reflection",
    "summary": "Large Vision-Language Models (VLMs) have demonstrated significant potential on complex visual understanding tasks through iterative optimization methods.However, these models generally lack effective self-correction mechanisms, making it difficult for them to independently rectify cognitive biases. Consequently, during multi-turn revisions, they often fall into repetitive and ineffective attempts, failing to achieve stable improvements in answer quality.To address this issue, we propose a novel iterative self-correction framework that endows models with two key capabilities: Capability Reflection and Memory Reflection. This framework guides the model to first diagnose errors and generate a correction plan via Capability Reflection, then leverage Memory Reflection to review past attempts to avoid repetition and explore new solutions, and finally, optimize the answer through rigorous re-reasoning. Experiments on the challenging OCRBench v2 benchmark show that OCR-Agent outperforms the current open-source SOTA model InternVL3-8B by +2.0 on English and +1.2 on Chinese subsets, while achieving state-of-the-art results in Visual Understanding (79.9) and Reasoning (66.5) - surpassing even larger fine-tuned models. Our method demonstrates that structured, self-aware reflection can significantly enhance VLMs' reasoning robustness without additional training. Code: https://github.com/AIGeeksGroup/OCR-Agent.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21053.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec877bb93654d4ca5c92e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
      "fullname": "Zeyu Zhang",
      "name": "SteveZeyuZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "686804a93cd67b3369df8182",
      "name": "AIGeeksGroup",
      "fullname": "AI Geeks",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/cyfudFg0OeqgwcLHwjeCc.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.21042",
      "authors": [
        {
          "_id": "699e984fdfbcf0b800aecbbc",
          "name": "Bonan Liu",
          "hidden": false
        },
        {
          "_id": "699e984fdfbcf0b800aecbbd",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "699e984fdfbcf0b800aecbbe",
          "name": "Bingbing Meng",
          "hidden": false
        },
        {
          "_id": "699e984fdfbcf0b800aecbbf",
          "name": "Han Wang",
          "hidden": false
        },
        {
          "_id": "699e984fdfbcf0b800aecbc0",
          "name": "Hanshuo Zhang",
          "hidden": false
        },
        {
          "_id": "699e984fdfbcf0b800aecbc1",
          "name": "Chengping Wang",
          "hidden": false
        },
        {
          "_id": "699e984fdfbcf0b800aecbc2",
          "name": "Daji Ergu",
          "hidden": false
        },
        {
          "_id": "699e984fdfbcf0b800aecbc3",
          "name": "Ying Cai",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-24T16:02:49.000Z",
      "submittedOnDailyAt": "2026-02-25T04:06:26.796Z",
      "title": "OmniOCR: Generalist OCR for Ethnic Minority Languages",
      "submittedOnDailyBy": {
        "_id": "64ec877bb93654d4ca5c92e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
        "isPro": false,
        "fullname": "Zeyu Zhang",
        "user": "SteveZeyuZhang",
        "type": "user"
      },
      "summary": "Optical character recognition (OCR) has advanced rapidly with deep learning and multimodal models, yet most methods focus on well-resourced scripts such as Latin and Chinese. Ethnic minority languages remain underexplored due to complex writing systems, scarce annotations, and diverse historical and modern forms, making generalization in low-resource or zero-shot settings challenging. To address these challenges, we present OmniOCR, a universal framework for ethnic minority scripts. OmniOCR introduces Dynamic Low-Rank Adaptation (Dynamic LoRA) to allocate model capacity across layers and scripts, enabling effective adaptation while preserving knowledge.A sparsity regularization prunes redundant updates, ensuring compact and efficient adaptation without extra inference cost. Evaluations on TibetanMNIST, Shui, ancient Yi, and Dongba show that OmniOCR outperforms zero-shot foundation models and standard post training, achieving state-of-the-art accuracy with superior parameter efficiency, and compared with the state-of-the-art baseline models, it improves accuracy by 39%-66% on these four datasets. Code: https://github.com/AIGeeksGroup/OmniOCR.",
      "upvotes": 0,
      "discussionId": "699e9850dfbcf0b800aecbc4",
      "githubRepo": "https://github.com/AIGeeksGroup/OmniOCR",
      "githubRepoAddedBy": "user",
      "ai_summary": "OmniOCR presents a universal framework for ethnic minority scripts using Dynamic LoRA and sparsity regularization to achieve state-of-the-art accuracy with improved parameter efficiency in low-resource settings.",
      "ai_keywords": [
        "Dynamic Low-Rank Adaptation",
        "Dynamic LoRA",
        "sparsity regularization",
        "zero-shot foundation models",
        "post training",
        "parameter efficiency"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "686804a93cd67b3369df8182",
        "name": "AIGeeksGroup",
        "fullname": "AI Geeks",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/cyfudFg0OeqgwcLHwjeCc.png"
      }
    },
    "publishedAt": "2026-02-24T11:02:49.000Z",
    "title": "OmniOCR: Generalist OCR for Ethnic Minority Languages",
    "summary": "Optical character recognition (OCR) has advanced rapidly with deep learning and multimodal models, yet most methods focus on well-resourced scripts such as Latin and Chinese. Ethnic minority languages remain underexplored due to complex writing systems, scarce annotations, and diverse historical and modern forms, making generalization in low-resource or zero-shot settings challenging. To address these challenges, we present OmniOCR, a universal framework for ethnic minority scripts. OmniOCR introduces Dynamic Low-Rank Adaptation (Dynamic LoRA) to allocate model capacity across layers and scripts, enabling effective adaptation while preserving knowledge.A sparsity regularization prunes redundant updates, ensuring compact and efficient adaptation without extra inference cost. Evaluations on TibetanMNIST, Shui, ancient Yi, and Dongba show that OmniOCR outperforms zero-shot foundation models and standard post training, achieving state-of-the-art accuracy with superior parameter efficiency, and compared with the state-of-the-art baseline models, it improves accuracy by 39%-66% on these four datasets. Code: https://github.com/AIGeeksGroup/OmniOCR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21042.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec877bb93654d4ca5c92e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
      "fullname": "Zeyu Zhang",
      "name": "SteveZeyuZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "686804a93cd67b3369df8182",
      "name": "AIGeeksGroup",
      "fullname": "AI Geeks",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/cyfudFg0OeqgwcLHwjeCc.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.20903",
      "authors": [
        {
          "_id": "699ea351dfbcf0b800aecc1a",
          "name": "Hanshen Zhu",
          "hidden": false
        },
        {
          "_id": "699ea351dfbcf0b800aecc1b",
          "name": "Yuliang Liu",
          "hidden": false
        },
        {
          "_id": "699ea351dfbcf0b800aecc1c",
          "name": "Xuecheng Wu",
          "hidden": false
        },
        {
          "_id": "699ea351dfbcf0b800aecc1d",
          "name": "An-Lan Wang",
          "hidden": false
        },
        {
          "_id": "699ea351dfbcf0b800aecc1e",
          "name": "Hao Feng",
          "hidden": false
        },
        {
          "_id": "699ea351dfbcf0b800aecc1f",
          "name": "Dingkang Yang",
          "hidden": false
        },
        {
          "_id": "699ea351dfbcf0b800aecc20",
          "name": "Chao Feng",
          "hidden": false
        },
        {
          "_id": "699ea351dfbcf0b800aecc21",
          "name": "Can Huang",
          "hidden": false
        },
        {
          "_id": "699ea351dfbcf0b800aecc22",
          "name": "Jingqun Tang",
          "hidden": false
        },
        {
          "_id": "699ea351dfbcf0b800aecc23",
          "name": "Xiang Bai",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-24T13:40:23.000Z",
      "submittedOnDailyAt": "2026-02-25T04:57:08.395Z",
      "title": "TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering",
      "submittedOnDailyBy": {
        "_id": "662213b3c819b7ce3d41f987",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662213b3c819b7ce3d41f987/cSdLQy5YbbaTokzKXMtrS.jpeg",
        "isPro": false,
        "fullname": "Hanshen Zhu",
        "user": "CIawevy",
        "type": "user"
      },
      "summary": "Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation, where even advanced models frequently produce text with structural anomalies such as distortion, blurriness, and misalignment. However, we find that leading MLLMs and specialist OCR models largely fail to perceive these structural anomalies, creating a critical bottleneck for both VTR evaluation and RL-based optimization. As a result, even state-of-the-art generators (e.g., SeedDream4.0, Qwen-Image) still struggle to render structurally faithful text. To address this, we propose TextPecker, a plug-and-play structural anomaly perceptive RL strategy that mitigates noisy reward signals and works with any textto-image generator. To enable this capability, we construct a recognition dataset with character-level structural-anomaly annotations and develop a stroke-editing synthesis engine to expand structural-error coverage. Experiments show that TextPecker consistently improves diverse text-to-image models; even on the well-optimized Qwen-Image, it significantly yields average gains of 4% in structural fidelity and 8.7% in semantic alignment for Chinese text rendering, establishing a new state-of-the-art in high-fidelity VTR. Our work fills a gap in VTR optimization, providing a foundational step towards reliable and structural faithful visual text generation.",
      "upvotes": 0,
      "discussionId": "699ea351dfbcf0b800aecc24",
      "projectPage": "https://github.com/CIawevy/TextPecker",
      "githubRepo": "https://github.com/CIawevy/TextPecker",
      "githubRepoAddedBy": "user",
      "ai_summary": "TextPecker enhances visual text rendering by addressing structural anomalies through a reinforcement learning approach that improves text-to-image generation quality and fidelity.",
      "ai_keywords": [
        "visual text rendering",
        "text-to-image generation",
        "structural anomalies",
        "reinforcement learning",
        "textPecker",
        "character-level annotations",
        "stroke-editing synthesis",
        "structural fidelity",
        "semantic alignment"
      ],
      "githubStars": 3
    },
    "publishedAt": "2026-02-24T08:40:23.000Z",
    "title": "TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering",
    "summary": "Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation, where even advanced models frequently produce text with structural anomalies such as distortion, blurriness, and misalignment. However, we find that leading MLLMs and specialist OCR models largely fail to perceive these structural anomalies, creating a critical bottleneck for both VTR evaluation and RL-based optimization. As a result, even state-of-the-art generators (e.g., SeedDream4.0, Qwen-Image) still struggle to render structurally faithful text. To address this, we propose TextPecker, a plug-and-play structural anomaly perceptive RL strategy that mitigates noisy reward signals and works with any textto-image generator. To enable this capability, we construct a recognition dataset with character-level structural-anomaly annotations and develop a stroke-editing synthesis engine to expand structural-error coverage. Experiments show that TextPecker consistently improves diverse text-to-image models; even on the well-optimized Qwen-Image, it significantly yields average gains of 4% in structural fidelity and 8.7% in semantic alignment for Chinese text rendering, establishing a new state-of-the-art in high-fidelity VTR. Our work fills a gap in VTR optimization, providing a foundational step towards reliable and structural faithful visual text generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20903.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "662213b3c819b7ce3d41f987",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662213b3c819b7ce3d41f987/cSdLQy5YbbaTokzKXMtrS.jpeg",
      "fullname": "Hanshen Zhu",
      "name": "CIawevy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.20540",
      "authors": [
        {
          "_id": "699e89c1dfbcf0b800aecb9b",
          "name": "Minseop Kim",
          "hidden": false
        },
        {
          "_id": "699e89c1dfbcf0b800aecb9c",
          "name": "Takhyeong Kim",
          "hidden": false
        },
        {
          "_id": "699e89c1dfbcf0b800aecb9d",
          "name": "Taekhyun Park",
          "hidden": false
        },
        {
          "_id": "699e89c1dfbcf0b800aecb9e",
          "name": "Hanbyeol Park",
          "hidden": false
        },
        {
          "_id": "699e89c1dfbcf0b800aecb9f",
          "name": "Hyerim Bae",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-24T04:38:31.000Z",
      "submittedOnDailyAt": "2026-02-25T03:47:43.183Z",
      "title": "Generative AI and Machine Learning Collaboration for Container Dwell Time Prediction via Data Standardization",
      "submittedOnDailyBy": {
        "_id": "64ba75761d0a5a5760874197",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ba75761d0a5a5760874197/5V3jxX-vim-wMaU2Zjz5h.jpeg",
        "isPro": false,
        "fullname": "TaekHyunPark",
        "user": "Thrillcrazyer",
        "type": "user"
      },
      "summary": "Import container dwell time (ICDT) prediction is a key task for improving productivity in container terminals, as accurate predictions enable the reduction of container re-handling operations by yard cranes. Achieving this objective requires accurately predicting the dwell time of individual containers. However, the primary determinants of dwell time-owner information and cargo information-are recorded as unstructured text, which limits their effective use in machine learning models. This study addresses this limitation by proposing a collaborative framework that integrates generative artificial intelligence (Gen AI) with machine learning. The proposed framework employs Gen AI to standardize unstructured information into standard international codes, with dynamic re-prediction triggered by electronic data interchange state updates, enabling the machine learning model to predict ICDT accurately. Extensive experiments conducted on real container terminal data demonstrate that the proposed methodology achieves a 13.88% improvement in mean absolute error compared to conventional models that do not utilize standardized information. Furthermore, applying the improved predictions to container stacking strategies achieves up to 14.68% reduction in the number of relocations, thereby empirically validating the potential of Gen AI to enhance productivity in container terminal operations. Overall, this study provides both technical and methodological insights into the adoption of Gen AI in port logistics and its effectiveness.",
      "upvotes": 0,
      "discussionId": "699e89c2dfbcf0b800aecba0",
      "ai_summary": "A collaborative framework integrating generative artificial intelligence with machine learning improves container dwell time prediction by standardizing unstructured text data, leading to reduced rehandling operations in container terminals.",
      "ai_keywords": [
        "generative artificial intelligence",
        "machine learning",
        "unstructured text",
        "electronic data interchange",
        "container dwell time",
        "re-handling operations",
        "container stacking strategies"
      ],
      "organization": {
        "_id": "6902caeadf78e6ca12c2a398",
        "name": "BAELABPNU",
        "fullname": "BIGDATA ANALYTICS ENGINEERING LAB, Pusan National University, Busan, Korea",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6902c9cb9427990a4948a33e/L59exY-PO66fQXk3lNwQ-.png"
      }
    },
    "publishedAt": "2026-02-23T23:38:31.000Z",
    "title": "Generative AI and Machine Learning Collaboration for Container Dwell Time Prediction via Data Standardization",
    "summary": "Import container dwell time (ICDT) prediction is a key task for improving productivity in container terminals, as accurate predictions enable the reduction of container re-handling operations by yard cranes. Achieving this objective requires accurately predicting the dwell time of individual containers. However, the primary determinants of dwell time-owner information and cargo information-are recorded as unstructured text, which limits their effective use in machine learning models. This study addresses this limitation by proposing a collaborative framework that integrates generative artificial intelligence (Gen AI) with machine learning. The proposed framework employs Gen AI to standardize unstructured information into standard international codes, with dynamic re-prediction triggered by electronic data interchange state updates, enabling the machine learning model to predict ICDT accurately. Extensive experiments conducted on real container terminal data demonstrate that the proposed methodology achieves a 13.88% improvement in mean absolute error compared to conventional models that do not utilize standardized information. Furthermore, applying the improved predictions to container stacking strategies achieves up to 14.68% reduction in the number of relocations, thereby empirically validating the potential of Gen AI to enhance productivity in container terminal operations. Overall, this study provides both technical and methodological insights into the adoption of Gen AI in port logistics and its effectiveness.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20540.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ba75761d0a5a5760874197",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ba75761d0a5a5760874197/5V3jxX-vim-wMaU2Zjz5h.jpeg",
      "fullname": "TaekHyunPark",
      "name": "Thrillcrazyer",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6902caeadf78e6ca12c2a398",
      "name": "BAELABPNU",
      "fullname": "BIGDATA ANALYTICS ENGINEERING LAB, Pusan National University, Busan, Korea",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6902c9cb9427990a4948a33e/L59exY-PO66fQXk3lNwQ-.png"
    },
    "isAuthorParticipating": false
  }
]